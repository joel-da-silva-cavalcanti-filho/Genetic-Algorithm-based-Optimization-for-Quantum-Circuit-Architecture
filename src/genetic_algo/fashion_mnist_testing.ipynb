{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "59442a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ansatz_simulation_class import AnsatzSimulation\n",
    "import torch\n",
    "import math\n",
    "from genetic_quantum import QuantumModel, QuanvLayer\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import make_grid\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "import pandas as pd\n",
    "from patch_making import PatchExtraction, Quanv_2d\n",
    "from torch import tensor\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0efd0227",
   "metadata": {},
   "source": [
    "### Defining Hybrid Module class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b9f0e2cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class L2NormalizationLayer(nn.Module):\n",
    "    def __init__(self, dim=1, eps=1e-12):\n",
    "        super(L2NormalizationLayer, self).__init__()\n",
    "        self.dim = dim\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.normalize(x, p=2, dim=self.dim, eps=self.eps)\n",
    "\n",
    "class HybridModel(nn.Module):\n",
    "    def __init__(self, n_qubits, patch_size, chromosome, num_classes, input_size, ansatz_parameters, mode='2d'):\n",
    "        super().__init__()\n",
    "        self.quanv_layer = QuanvLayer(\n",
    "            n_qubits=n_qubits,\n",
    "            patch_size=patch_size,\n",
    "            chromosome=chromosome,\n",
    "            parameters = ansatz_parameters,\n",
    "            mode=mode\n",
    "        )\n",
    "        feature_size = input_size\n",
    "        self.fc1 = nn.Linear(feature_size, 64)\n",
    "        self.fc2 = nn.Linear(64, num_classes)\n",
    "        self.norm = nn.LayerNorm(feature_size)\n",
    "        #self.conv1 = nn.Conv2d(in_channels = 1, output_channels=16, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=patch_size, stride=patch_size)\n",
    "        self.fc = nn.Linear(feature_size, num_classes)\n",
    "        self.l2norm = L2NormalizationLayer(dim=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        print(\"Passing through quanvolution layer...\")\n",
    "        start_time = time.perf_counter()\n",
    "        x = self.quanv_layer.forward(x)\n",
    "        end_time = time.perf_counter()\n",
    "        print(f'Quanvolution processing time: {end_time - start_time}')  \n",
    "        x = x.flatten(start_dim=1)\n",
    "        #print(x.shape)\n",
    "        x = self.l2norm(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2caf39e",
   "metadata": {},
   "source": [
    "### Instantiating genetic ansatz model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609bea41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HybridModel(\n",
       "  (quanv_layer): QuanvLayer()\n",
       "  (fc1): Linear(in_features=480, out_features=64, bias=True)\n",
       "  (fc2): Linear(in_features=64, out_features=2, bias=True)\n",
       "  (norm): LayerNorm((480,), eps=1e-05, elementwise_affine=True)\n",
       "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (fc): Linear(in_features=480, out_features=2, bias=True)\n",
       "  (l2norm): L2NormalizationLayer()\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_qubits = 4\n",
    "max_patches = 50\n",
    "patch_size = 2\n",
    "batch_size = 100\n",
    "num_classes = 2\n",
    "input_size = max_patches * n_qubits\n",
    "num_epochs = 4\n",
    "toy_chromosome = [['ctrl_0', 'trgt_0', 'ctrl_1', 'trgt_1'], [None, 'ctrl_0', 'trgt_0', None], ['ry_gate', 'rz_gate', 'ry_gate', 'rx_gate'],['trgt_0', 'ctrl_0', 'trgt_1', 'ctrl_1'], ['rz_gate', 'trgt_0', 'ctrl_0', 'ry_gate']]\n",
    "#simple_fc = ClassicalComponent(num_classes, input_size)\n",
    "params = torch.rand(6)*math.pi\n",
    "hqcnn = HybridModel(n_qubits, patch_size, toy_chromosome, num_classes, input_size, params)\n",
    "hqcnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ceb8ebb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters in the random circuit as angles: tensor([ 95.8854,  50.3609,  59.7582,  16.9121, 143.7999, 133.3029])\n"
     ]
    }
   ],
   "source": [
    "print(f'Parameters in the random circuit as angles: {torch.rad2deg(params)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee926d7",
   "metadata": {},
   "source": [
    "### Getting data and sampling it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f78aa0d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SampledDataset4Training(Dataset):\n",
    "    def __init__(self, dataset, target_classes, transform=None):\n",
    "        images, labels = zip(*dataset)\n",
    "        labels = [0 if label == target_classes[0] else 1 for label in tqdm(labels, desc='labeling')]\n",
    "        self.labels = tensor(labels)\n",
    "        self.images = torch.stack(images)\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        if torch.is_tensor(index):\n",
    "            index = index.tolist()\n",
    "            \n",
    "        \n",
    "        if self.transform:\n",
    "            sample = self.transform(self.images[index])\n",
    "        else:\n",
    "            sample = self.images[index]\n",
    "        \n",
    "        \n",
    "        return sample, self.labels[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da7dfc49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_classes(target_classes, dataset):\n",
    "    return [dataset.__getitem__(index) for index in tqdm(range(dataset.__len__()), desc='getting the target classes') if dataset.__getitem__(index)[1] in target_classes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f017c858",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_data(dataset, classes, sample_size):\n",
    "    class_one = 0\n",
    "    class_two = 0\n",
    "    binary_dataset = []\n",
    "\n",
    "    for image in tqdm(dataset, desc='sampling data'):\n",
    "        if image[1] == classes[0] and class_one < sample_size:\n",
    "            binary_dataset.append(image)\n",
    "            class_one +=1 \n",
    "        elif image[1] == classes[1] and class_two < sample_size:\n",
    "            binary_dataset.append(image)\n",
    "            class_two += 1\n",
    "            \n",
    "        if class_one == sample_size and class_two == sample_size:\n",
    "            break\n",
    "    \n",
    "    return binary_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0dbbd103",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(dataset_class, transform_routine, target_classes, sample_size):\n",
    "    training_dataset = dataset_class(root='./data', train=True, download=True, transform=transform_routine)\n",
    "    validation_dataset = dataset_class(root='./data', train=False, transform=transform_routine, download=True)\n",
    "    \n",
    "    reduced_training_dataset = sample_classes(target_classes, training_dataset)\n",
    "    reduced_validation_dataset = sample_classes(target_classes, validation_dataset)\n",
    "    \n",
    "    sampled_training_dataset = sample_data(reduced_training_dataset, target_classes, sample_size)\n",
    "    sampled_validation_dataset = sample_data(reduced_validation_dataset, target_classes, sample_size)\n",
    "    \n",
    "    return SampledDataset4Training(sampled_training_dataset, target_classes), SampledDataset4Training(sampled_validation_dataset, target_classes) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba0d7c2e",
   "metadata": {},
   "source": [
    "#### Testing with Fashion MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "61e937b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60000/60000 [02:10<00:00, 461.41it/s]\n",
      "100%|██████████| 10000/10000 [00:14<00:00, 680.76it/s]\n",
      "sampling data:   4%|▎         | 426/12000 [00:00<00:00, 427049.12it/s]\n",
      "sampling data:  21%|██        | 418/2000 [00:00<?, ?it/s]\n",
      "labeling: 100%|██████████| 400/400 [00:00<?, ?it/s]\n",
      "labeling: 100%|██████████| 400/400 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    PatchExtraction(patch_size, max_patches)])\n",
    "\n",
    "target_classes = [1, 4]\n",
    "\n",
    "sample_size = 200\n",
    "\n",
    "training_data, validation_data = create_dataset(datasets.FashionMNIST, transform, target_classes, sample_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c90f06",
   "metadata": {},
   "source": [
    "##### Training my model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "01040e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, num_epochs, optimizer, loss_fn, filepath):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  # Set model to training mode\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        epoch_loss = 0.0\n",
    "        total = 0\n",
    "        \n",
    "        progress_bar = tqdm(enumerate(train_loader), total=len(train_loader), desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        for i, (inputs, labels) in progress_bar:\n",
    "        \n",
    "            optimizer.zero_grad()  # Zero out previous gradients\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_fn(outputs, labels)  # Calculate loss\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "            \n",
    "            loss.backward()  # Backpropagate to calculate gradients\n",
    "            optimizer.step() # Update weights\n",
    "            \n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_loader)}], Loss: {running_loss/(i+1):.4f}, Acc: {correct / total:.4f}\")\n",
    "            \n",
    "        epoch_loss = running_loss / len(train_loader)\n",
    "        epoch_acc = correct / total\n",
    "        print(f\"Epoch {epoch+1}: Train Loss={epoch_loss:.4f}, Train Acc={epoch_acc:.4f}\")\n",
    "            # Print every 10 batches\n",
    "            \n",
    "    torch.save(model.state_dict(), filepath)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3c2e6dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#quantum_processing = transforms.Compose([Quanv_2d(n_qubits, toy_chromosome)])\n",
    "#training_zeros_ones = SampledDataset4Training(binary_dataset)\n",
    "training_loader = DataLoader(training_data, batch_size, shuffle=True)\n",
    "validation_loader = DataLoader(validation_data, batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "156bdadc",
   "metadata": {},
   "outputs": [],
   "source": [
    "post_processing = [label for (data, label) in validation_loader]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "eea3b9e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1,\n",
       "         1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0,\n",
       "         1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,\n",
       "         1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "         0, 0, 1, 0]),\n",
       " tensor([1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0,\n",
       "         1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0,\n",
       "         0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0,\n",
       "         1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1,\n",
       "         0, 1, 0, 0]),\n",
       " tensor([1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1,\n",
       "         0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1,\n",
       "         1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0,\n",
       "         1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0,\n",
       "         0, 0, 0, 0]),\n",
       " tensor([1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1,\n",
       "         0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1,\n",
       "         1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1,\n",
       "         0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1,\n",
       "         1, 0, 0, 0])]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "post_processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "970ae5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(hqcnn.parameters(), lr=0.01, weight_decay=1e-4)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "#loss = loss_fn(output, mnist_labels)\n",
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "10146a35",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/4:   0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passing through quanvolution layer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/4:  25%|██▌       | 1/4 [00:03<00:11,  3.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quanvolution processing time: 3.9030137000372633\n",
      "Epoch [1/4], Step [1/4], Loss: 0.6619, Acc: 0.8000\n",
      "Passing through quanvolution layer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/4:  50%|█████     | 2/4 [00:07<00:07,  3.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quanvolution processing time: 3.8075078999972902\n",
      "Epoch [1/4], Step [2/4], Loss: 0.6869, Acc: 0.6300\n",
      "Passing through quanvolution layer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/4:  75%|███████▌  | 3/4 [00:11<00:03,  3.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quanvolution processing time: 3.809125700034201\n",
      "Epoch [1/4], Step [3/4], Loss: 0.6805, Acc: 0.5800\n",
      "Passing through quanvolution layer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/4: 100%|██████████| 4/4 [00:15<00:00,  3.80s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quanvolution processing time: 3.6741393000120297\n",
      "Epoch [1/4], Step [4/4], Loss: 0.6721, Acc: 0.6650\n",
      "Epoch 1: Train Loss=0.6721, Train Acc=0.6650\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/4:   0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passing through quanvolution layer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/4:  25%|██▌       | 1/4 [00:03<00:11,  3.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quanvolution processing time: 3.8795704999938607\n",
      "Epoch [2/4], Step [1/4], Loss: 0.6356, Acc: 0.5900\n",
      "Passing through quanvolution layer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/4:  50%|█████     | 2/4 [00:07<00:07,  3.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quanvolution processing time: 3.621121199976187\n",
      "Epoch [2/4], Step [2/4], Loss: 0.6796, Acc: 0.5100\n",
      "Passing through quanvolution layer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/4:  75%|███████▌  | 3/4 [00:11<00:03,  3.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quanvolution processing time: 3.901154100021813\n",
      "Epoch [2/4], Step [3/4], Loss: 0.6710, Acc: 0.5100\n",
      "Passing through quanvolution layer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/4: 100%|██████████| 4/4 [00:15<00:00,  3.80s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quanvolution processing time: 3.797614999988582\n",
      "Epoch [2/4], Step [4/4], Loss: 0.6647, Acc: 0.6000\n",
      "Epoch 2: Train Loss=0.6647, Train Acc=0.6000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/4:   0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passing through quanvolution layer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/4:  25%|██▌       | 1/4 [00:03<00:10,  3.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quanvolution processing time: 3.659932399983518\n",
      "Epoch [3/4], Step [1/4], Loss: 0.6604, Acc: 0.5200\n",
      "Passing through quanvolution layer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/4:  50%|█████     | 2/4 [00:07<00:07,  3.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quanvolution processing time: 3.7733293999917805\n",
      "Epoch [3/4], Step [2/4], Loss: 0.6730, Acc: 0.5000\n",
      "Passing through quanvolution layer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/4:  75%|███████▌  | 3/4 [00:11<00:03,  3.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quanvolution processing time: 3.6863480000174604\n",
      "Epoch [3/4], Step [3/4], Loss: 0.6578, Acc: 0.5700\n",
      "Passing through quanvolution layer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/4: 100%|██████████| 4/4 [00:14<00:00,  3.74s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quanvolution processing time: 3.808891399996355\n",
      "Epoch [3/4], Step [4/4], Loss: 0.6500, Acc: 0.6550\n",
      "Epoch 3: Train Loss=0.6500, Train Acc=0.6550\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/4:   0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passing through quanvolution layer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/4:  25%|██▌       | 1/4 [00:04<00:12,  4.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quanvolution processing time: 4.091181999945547\n",
      "Epoch [4/4], Step [1/4], Loss: 0.6416, Acc: 0.5200\n",
      "Passing through quanvolution layer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/4:  50%|█████     | 2/4 [00:08<00:08,  4.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quanvolution processing time: 4.143171800009441\n",
      "Epoch [4/4], Step [2/4], Loss: 0.6305, Acc: 0.5250\n",
      "Passing through quanvolution layer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/4:  75%|███████▌  | 3/4 [00:12<00:04,  4.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quanvolution processing time: 4.060235599987209\n",
      "Epoch [4/4], Step [3/4], Loss: 0.6360, Acc: 0.4867\n",
      "Passing through quanvolution layer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/4: 100%|██████████| 4/4 [00:16<00:00,  4.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quanvolution processing time: 4.161958799988497\n",
      "Epoch [4/4], Step [4/4], Loss: 0.6297, Acc: 0.5775\n",
      "Epoch 4: Train Loss=0.6297, Train Acc=0.5775\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "filepath = \"C:/Users/speak/Genetic-Algorithm-based-Optimization-for-Quantum-Circuit-Synthesis/src/models/fashion_MNIST/hqcnn_fashionMNIST_weights_with_random_PQC.pth\"\n",
    "\n",
    "train_model(hqcnn, training_loader, num_epochs, optimizer, loss_fn, filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "716c2079",
   "metadata": {},
   "source": [
    "#### Evaluating feature space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f2fc876a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: tensor(0.4046)\n",
      "std: tensor(0.4424)\n"
     ]
    }
   ],
   "source": [
    "mean = 0.\n",
    "meansq = 0.\n",
    "for (data, label) in training_loader:\n",
    "    mean = data.mean()\n",
    "    meansq = (data**2).mean()\n",
    "\n",
    "std = torch.sqrt(meansq - mean**2)\n",
    "print(\"mean: \" + str(mean))\n",
    "print(\"std: \" + str(std))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "08f4aba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "post_processing = [data for (data, label) in training_loader]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8276ca39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 100, 120, 4])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "post_process = torch.stack(post_processing).squeeze(0)\n",
    "post_process.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3fe35fff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.4749)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_p = torch.mean(post_process)\n",
    "mean_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2bfbc050",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1372)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_diffs = post_process - mean_p\n",
    "p_var = torch.mean(torch.pow(p_diffs, 2.0))\n",
    "p_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3a8f31c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.3704)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_std = torch.pow(p_var, 0.5)\n",
    "p_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "545eb3c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.7806)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phi_n = F.normalize(post_process.flatten(start_dim=1), dim=1)\n",
    "cos_sim = phi_n @ phi_n.T\n",
    "cos_sim.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f07947b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e360073",
   "metadata": {},
   "source": [
    "### Testing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4db25f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, test_loader, hyperparams, num_epochs, optimizer, loss_fn, output_file):\n",
    "    model.eval()\n",
    "    \n",
    "    total_loss = 0.0\n",
    "    all_labels = []\n",
    "    all_predictions = []\n",
    "    all_probs = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            \n",
    "            outputs = model(images)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            total_loss += loss.item(outputs, dim=1)[:, 1]\n",
    "            \n",
    "            probs = torch.softmax()\n",
    "            all_probs.extend(probs)\n",
    "            all_predictions.extend(outputs.argmax(dim=1).cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    accuracy = accuracy_score(all_labels, all_predictions)\n",
    "    precision = precision_score(all_labels, all_predictions, zero_division=0)\n",
    "    recall = recall_score(all_labels, all_predictions, zero_division=0)\n",
    "    f1 = f1_score(all_labels, all_predictions, zero_division=0)\n",
    "    try:\n",
    "        auc = roc_auc_score(all_labels, all_probs)\n",
    "    except ValueError:\n",
    "        auc = float(\"nan\")\n",
    "\n",
    "    avg_loss = total_loss / len(test_loader)\n",
    "    \n",
    "    with open(output_file, \"a\") as f:\n",
    "        f.write(f\"\\n[Quantum Model Testing - {time.perf_counter()}]\\n\")\n",
    "        f.write(f\"Hyperparameters: {hyperparams}\\n\")\n",
    "        f.write(f\"Test Loss: {avg_loss:.4f}\\n\")\n",
    "        f.write(f\"Accuracy: {accuracy:.4f}\\n\")\n",
    "        f.write(f\"Precision: {precision:.4f}\\n\")\n",
    "        f.write(f\"Recall: {recall:.4f}\\n\")\n",
    "        f.write(f\"F1 Score: {f1:.4f}\\n\")\n",
    "        f.write(f\"AUC: {auc:.4f}\\n\")\n",
    "        f.write(\"-\" * 50 + \"\\n\")\n",
    "\n",
    "    print(f\"Test Loss: {avg_loss:.4f}\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1: {f1:.4f}, AUC: {auc:.4f}\")\n",
    "\n",
    "    return avg_loss, accuracy, precision, recall, f1, auc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a319884f",
   "metadata": {},
   "source": [
    "### Validating model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d23f35ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_model(model, dataloader, criterion, device):\n",
    "    model.eval()    \n",
    "    model.to(device)\n",
    "\n",
    "    total_loss = 0.0\n",
    "    all_labels = []\n",
    "    all_predictions = []\n",
    "    all_probs = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(dataloader, desc=\"Validation\"):\n",
    "#            images = images.to(device)\n",
    "#            labels = labels.squeeze().to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            probs = torch.softmax(outputs, dim=1)[:, 1].cpu().numpy() if outputs.shape[1] > 1 else torch.softmax(outputs, dim=1)[:, 0].cpu().numpy()\n",
    "            all_probs.extend(probs)\n",
    "            all_predictions.extend(outputs.argmax(dim=1).cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    accuracy = accuracy_score(all_labels, all_predictions)\n",
    "    precision = precision_score(all_labels, all_predictions, zero_division=0)\n",
    "    recall = recall_score(all_labels, all_predictions, zero_division=0)\n",
    "    f1 = f1_score(all_labels, all_predictions, zero_division=0)\n",
    "    try:\n",
    "        auc = roc_auc_score(all_labels, all_probs)\n",
    "    except ValueError:\n",
    "        auc = float(\"nan\")\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    print(f\"Validation Loss: {avg_loss:.4f}\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1: {f1:.4f}, AUC: {auc:.4f}\")\n",
    "\n",
    "    return avg_loss, accuracy, precision, recall, f1, auc\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "58998e6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation:   0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passing through quanvolution layer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation:  25%|██▌       | 1/4 [00:03<00:11,  3.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quanvolution processing time: 3.846704400028102\n",
      "Passing through quanvolution layer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation:  50%|█████     | 2/4 [00:07<00:07,  3.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quanvolution processing time: 3.698145799979102\n",
      "Passing through quanvolution layer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation:  75%|███████▌  | 3/4 [00:11<00:03,  3.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quanvolution processing time: 3.9057699000113644\n",
      "Passing through quanvolution layer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 4/4 [00:16<00:00,  4.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quanvolution processing time: 4.819339300040156\n",
      "Validation Loss: 0.6129\n",
      "Accuracy: 0.7950, Precision: 0.9538, Recall: 0.6200, F1: 0.7515, AUC: 0.8943\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.6128852218389511,\n",
       " 0.795,\n",
       " 0.9538461538461539,\n",
       " 0.62,\n",
       " 0.7515151515151515,\n",
       " 0.894325)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validate_model(hqcnn, validation_loader, loss_fn, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b83bc791",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 200 into shape (28,28)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[54]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m     12\u001b[39m images, labels = \u001b[38;5;28mnext\u001b[39m(dataiter)\n\u001b[32m     14\u001b[39m img_grid = make_grid(images)\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m \u001b[43mmatplotlib_imshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_grid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mone_channel\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[54]\u001b[39m\u001b[32m, line 5\u001b[39m, in \u001b[36mmatplotlib_imshow\u001b[39m\u001b[34m(img, one_channel)\u001b[39m\n\u001b[32m      3\u001b[39m     img = img.mean(dim=\u001b[32m0\u001b[39m)\n\u001b[32m      4\u001b[39m img = img / \u001b[32m2\u001b[39m + \u001b[32m0.5\u001b[39m     \u001b[38;5;66;03m# unnormalize\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m npimg = \u001b[43mimg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m28\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m28\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m one_channel:\n\u001b[32m      7\u001b[39m     plt.imshow(npimg, cmap=\u001b[33m\"\u001b[39m\u001b[33mGreys\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mValueError\u001b[39m: cannot reshape array of size 200 into shape (28,28)"
     ]
    }
   ],
   "source": [
    "def matplotlib_imshow(img, one_channel=False):\n",
    "    if one_channel:\n",
    "        img = img.mean(dim=0)\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy().reshape(28, 28)\n",
    "    if one_channel:\n",
    "        plt.imshow(npimg, cmap=\"Greys\")\n",
    "    else:\n",
    "        plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "\n",
    "dataiter = iter(training_loader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "img_grid = make_grid(images)\n",
    "matplotlib_imshow(img_grid, one_channel=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".genetic_circuits (3.12.10)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
