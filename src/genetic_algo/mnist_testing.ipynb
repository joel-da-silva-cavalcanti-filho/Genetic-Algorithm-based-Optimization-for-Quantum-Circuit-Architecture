{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "59442a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ansatz_simulation_class import AnsatzSimulation\n",
    "import torch\n",
    "import math\n",
    "from genetic_quantum import QuantumModel, QuanvLayer\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import make_grid\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "import pandas as pd\n",
    "from patch_making import PatchExtraction, Quanv_2d\n",
    "from torch import tensor\n",
    "import matplotlib.pyplot as plt\n",
    "from sampled_cv_dataset import SampledDataset4Training\n",
    "import time\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0efd0227",
   "metadata": {},
   "source": [
    "### Defining Hyrbid Module class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b9f0e2cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class L2NormalizationLayer(nn.Module):\n",
    "    def __init__(self, dim=1, eps=1e-12):\n",
    "        super(L2NormalizationLayer, self).__init__()\n",
    "        self.dim = dim\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.normalize(x, p=2, dim=self.dim, eps=self.eps)\n",
    "\n",
    "class HybridModel(nn.Module):\n",
    "    def __init__(self, n_qubits, patch_size, chromosome, num_classes, input_size, ansatz_parameters, mode='2d'):\n",
    "        super().__init__()\n",
    "        self.quanv_layer = QuanvLayer(\n",
    "            n_qubits=n_qubits,\n",
    "            patch_size=patch_size,\n",
    "            chromosome=chromosome,\n",
    "            parameters = ansatz_parameters,\n",
    "            mode=mode\n",
    "        )\n",
    "        feature_size = input_size\n",
    "        self.fc1 = nn.Linear(feature_size, 64)\n",
    "        self.fc2 = nn.Linear(64, num_classes)\n",
    "        self.norm = nn.LayerNorm(feature_size)\n",
    "        #self.conv1 = nn.Conv2d(in_channels = 1, output_channels=16, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=patch_size, stride=patch_size)\n",
    "        self.fc = nn.Linear(feature_size, num_classes)\n",
    "        self.l2norm = L2NormalizationLayer(dim=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        print(\"Passing through quanvolution layer...\")\n",
    "        start_time = time.perf_counter()\n",
    "        x = self.quanv_layer.forward(x)\n",
    "        end_time = time.perf_counter()\n",
    "        print(f'Quanvolution processing time: {end_time - start_time}')  \n",
    "        x = x.flatten(start_dim=1)\n",
    "        #print(x.shape)\n",
    "        x = self.l2norm(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2caf39e",
   "metadata": {},
   "source": [
    "### Instantiating genetic ansatz model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "609bea41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HybridModel(\n",
       "  (quanv_layer): QuanvLayer()\n",
       "  (fc1): Linear(in_features=320, out_features=64, bias=True)\n",
       "  (fc2): Linear(in_features=64, out_features=2, bias=True)\n",
       "  (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (fc): Linear(in_features=320, out_features=2, bias=True)\n",
       "  (l2norm): L2NormalizationLayer()\n",
       ")"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_qubits = 4\n",
    "max_patches = 80\n",
    "patch_size = 2\n",
    "batch_size = 100\n",
    "num_classes = 2\n",
    "input_size = max_patches * n_qubits\n",
    "num_epochs = 4\n",
    "toy_chromosome = [['ctrl_0', 'trgt_0', 'ctrl_1', 'trgt_1'], [None, 'ctrl_0', 'trgt_0', None], ['ry_gate', 'rz_gate', 'ry_gate', 'rx_gate'],['trgt_0', 'ctrl_0', 'trgt_1', 'ctrl_1'], ['rz_gate', 'trgt_0', 'ctrl_0', 'ry_gate']]\n",
    "#simple_fc = ClassicalComponent(num_classes, input_size)\n",
    "params = torch.rand(6)*math.pi\n",
    "hqcnn = HybridModel(n_qubits, patch_size, toy_chromosome, num_classes, input_size, params)\n",
    "hqcnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ceb8ebb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters in the random circuit: tensor([123.3380, 161.1354, 121.5233, 145.8758, 169.8532,  72.2481])\n"
     ]
    }
   ],
   "source": [
    "print(f'Parameters in the random circuit as angles: {torch.rad2deg(params)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee926d7",
   "metadata": {},
   "source": [
    "### Getting data and sampling it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "da7dfc49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_classes(target_classes, dataset):\n",
    "    return [dataset.__getitem__(index) for index in range(dataset.__len__()) if dataset.__getitem__(index)[1] in target_classes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f017c858",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_data(dataset, classes, sample_size):\n",
    "    class_one = 0\n",
    "    class_two = 0\n",
    "    binary_dataset = []\n",
    "\n",
    "    for image in dataset:\n",
    "        if image[1] == classes[0] and class_one < sample_size:\n",
    "            binary_dataset.append(image)\n",
    "            class_one +=1 \n",
    "        elif image[1] == classes[1] and class_two < sample_size:\n",
    "            binary_dataset.append(image)\n",
    "            class_two += 1\n",
    "            \n",
    "        if class_one == sample_size and class_two == sample_size:\n",
    "            break\n",
    "    \n",
    "    return binary_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0dbbd103",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(dataset_class, transform_routine, target_classes, sample_size):\n",
    "    training_dataset = dataset_class(root='./data', train=True, download=True, transform=transform_routine)\n",
    "    validation_dataset = dataset_class(root='./data', train=False, transform=transform_routine, download=True)\n",
    "    \n",
    "    reduced_training_dataset = sample_classes(target_classes, training_dataset)\n",
    "    reduced_validation_dataset = sample_classes(target_classes, validation_dataset)\n",
    "    \n",
    "    sampled_training_dataset = sample_data(reduced_training_dataset, target_classes, sample_size)\n",
    "    sampled_validation_dataset = sample_data(reduced_validation_dataset, target_classes, sample_size)\n",
    "    \n",
    "    return SampledDataset4Training(sampled_training_dataset), SampledDataset4Training(sampled_validation_dataset) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "61e937b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    PatchExtraction(patch_size, max_patches)])\n",
    "\n",
    "target_classes = [0, 1]\n",
    "\n",
    "sample_size = 200\n",
    "\n",
    "training_data, validation_data = create_dataset(datasets.MNIST, transform, target_classes, sample_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c90f06",
   "metadata": {},
   "source": [
    "### Training my model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "01040e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, num_epochs, optimizer, loss_fn, filepath):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  # Set model to training mode\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        epoch_loss = 0.0\n",
    "        total = 0\n",
    "        \n",
    "        progress_bar = tqdm(enumerate(train_loader), total=len(train_loader), desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        for i, (inputs, labels) in progress_bar:\n",
    "        \n",
    "            optimizer.zero_grad()  # Zero out previous gradients\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_fn(outputs, labels)  # Calculate loss\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "            \n",
    "            loss.backward()  # Backpropagate to calculate gradients\n",
    "            optimizer.step() # Update weights\n",
    "            \n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_loader)}], Loss: {running_loss/(i+1):.4f}, Acc: {correct / total:.4f}\")\n",
    "            \n",
    "        epoch_loss = running_loss / len(train_loader)\n",
    "        epoch_acc = correct / total\n",
    "        print(f\"Epoch {epoch+1}: Train Loss={epoch_loss:.4f}, Train Acc={epoch_acc:.4f}\")\n",
    "            # Print every 10 batches\n",
    "            \n",
    "    torch.save(model.state_dict(), filepath)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3c2e6dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#quantum_processing = transforms.Compose([Quanv_2d(n_qubits, toy_chromosome)])\n",
    "#training_zeros_ones = SampledDataset4Training(binary_dataset)\n",
    "training_loader = DataLoader(training_data, batch_size, shuffle=True)\n",
    "validation_loader = DataLoader(validation_data, batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "156bdadc",
   "metadata": {},
   "outputs": [],
   "source": [
    "post_processing = [label for (data, label) in validation_loader]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "eea3b9e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1,\n",
       "         1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1,\n",
       "         1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0,\n",
       "         0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1,\n",
       "         0, 1, 1, 0]),\n",
       " tensor([1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1,\n",
       "         1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0,\n",
       "         0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1,\n",
       "         0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1,\n",
       "         0, 1, 0, 1]),\n",
       " tensor([0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1,\n",
       "         0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1,\n",
       "         0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0,\n",
       "         1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0,\n",
       "         1, 0, 1, 0]),\n",
       " tensor([1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0,\n",
       "         1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0,\n",
       "         0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1,\n",
       "         0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1,\n",
       "         1, 1, 0, 1])]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "post_processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "970ae5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(hqcnn.parameters(), lr=0.01, weight_decay=1e-4)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "#loss = loss_fn(output, mnist_labels)\n",
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "10146a35",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/4:   0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passing through quanvolution layer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/4:  25%|██▌       | 1/4 [00:03<00:11,  3.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quanvolution processing time: 3.9074244999792427\n",
      "Epoch [1/4], Step [1/4], Loss: 0.6938, Acc: 0.5200\n",
      "Passing through quanvolution layer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/4:  50%|█████     | 2/4 [00:08<00:08,  4.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quanvolution processing time: 4.900008900032844\n",
      "Epoch [1/4], Step [2/4], Loss: 0.6931, Acc: 0.4850\n",
      "Passing through quanvolution layer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/4:  75%|███████▌  | 3/4 [00:13<00:04,  4.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quanvolution processing time: 4.559979699959513\n",
      "Epoch [1/4], Step [3/4], Loss: 0.6985, Acc: 0.4700\n",
      "Passing through quanvolution layer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/4: 100%|██████████| 4/4 [00:18<00:00,  4.69s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quanvolution processing time: 5.361737100000028\n",
      "Epoch [1/4], Step [4/4], Loss: 0.6947, Acc: 0.4850\n",
      "Epoch 1: Train Loss=0.6947, Train Acc=0.4850\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/4:   0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passing through quanvolution layer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/4:  25%|██▌       | 1/4 [00:04<00:14,  4.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quanvolution processing time: 4.930896400008351\n",
      "Epoch [2/4], Step [1/4], Loss: 0.6810, Acc: 0.9500\n",
      "Passing through quanvolution layer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/4:  50%|█████     | 2/4 [00:09<00:09,  4.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quanvolution processing time: 4.474312700040173\n",
      "Epoch [2/4], Step [2/4], Loss: 0.6796, Acc: 0.7200\n",
      "Passing through quanvolution layer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/4:  75%|███████▌  | 3/4 [00:13<00:04,  4.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quanvolution processing time: 4.501757399993949\n",
      "Epoch [2/4], Step [3/4], Loss: 0.6775, Acc: 0.7233\n",
      "Passing through quanvolution layer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/4: 100%|██████████| 4/4 [00:18<00:00,  4.70s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quanvolution processing time: 4.86482179997256\n",
      "Epoch [2/4], Step [4/4], Loss: 0.6754, Acc: 0.7625\n",
      "Epoch 2: Train Loss=0.6754, Train Acc=0.7625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/4:   0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passing through quanvolution layer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/4:  25%|██▌       | 1/4 [00:05<00:15,  5.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quanvolution processing time: 5.0903710999991745\n",
      "Epoch [3/4], Step [1/4], Loss: 0.6555, Acc: 0.5600\n",
      "Passing through quanvolution layer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/4:  50%|█████     | 2/4 [00:10<00:10,  5.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quanvolution processing time: 5.273983600025531\n",
      "Epoch [3/4], Step [2/4], Loss: 0.6640, Acc: 0.5050\n",
      "Passing through quanvolution layer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/4:  75%|███████▌  | 3/4 [00:14<00:04,  4.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quanvolution processing time: 4.560205599991605\n",
      "Epoch [3/4], Step [3/4], Loss: 0.6544, Acc: 0.5567\n",
      "Passing through quanvolution layer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/4: 100%|██████████| 4/4 [00:19<00:00,  4.93s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quanvolution processing time: 4.765281500003766\n",
      "Epoch [3/4], Step [4/4], Loss: 0.6494, Acc: 0.6500\n",
      "Epoch 3: Train Loss=0.6494, Train Acc=0.6500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/4:   0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passing through quanvolution layer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/4:  25%|██▌       | 1/4 [00:04<00:14,  4.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quanvolution processing time: 4.6721945000463165\n",
      "Epoch [4/4], Step [1/4], Loss: 0.6350, Acc: 0.6800\n",
      "Passing through quanvolution layer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/4:  50%|█████     | 2/4 [00:09<00:09,  4.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quanvolution processing time: 4.6229619999649\n",
      "Epoch [4/4], Step [2/4], Loss: 0.6152, Acc: 0.7650\n",
      "Passing through quanvolution layer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/4:  75%|███████▌  | 3/4 [00:14<00:04,  4.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quanvolution processing time: 4.77374360000249\n",
      "Epoch [4/4], Step [3/4], Loss: 0.6067, Acc: 0.7900\n",
      "Passing through quanvolution layer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/4: 100%|██████████| 4/4 [00:18<00:00,  4.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quanvolution processing time: 4.5730526999686845\n",
      "Epoch [4/4], Step [4/4], Loss: 0.6038, Acc: 0.8050\n",
      "Epoch 4: Train Loss=0.6038, Train Acc=0.8050\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "filepath = \"C:/Users/speak/Genetic-Algorithm-based-Optimization-for-Quantum-Circuit-Synthesis/src/models/hqcnn_mnist_weights_with_random_params.pth\"\n",
    "\n",
    "train_model(hqcnn, training_loader, num_epochs, optimizer, loss_fn, filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "716c2079",
   "metadata": {},
   "source": [
    "#### Evaluating feature space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f2fc876a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: tensor(0.4046)\n",
      "std: tensor(0.4424)\n"
     ]
    }
   ],
   "source": [
    "mean = 0.\n",
    "meansq = 0.\n",
    "for (data, label) in training_loader:\n",
    "    mean = data.mean()\n",
    "    meansq = (data**2).mean()\n",
    "\n",
    "std = torch.sqrt(meansq - mean**2)\n",
    "print(\"mean: \" + str(mean))\n",
    "print(\"std: \" + str(std))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "08f4aba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "post_processing = [data for (data, label) in training_loader]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8276ca39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 100, 50, 4])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "post_process = torch.stack(post_processing).squeeze(0)\n",
    "post_process.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3fe35fff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.4163)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_p = torch.mean(post_process)\n",
    "mean_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2bfbc050",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1968)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_diffs = post_process - mean_p\n",
    "p_var = torch.mean(torch.pow(p_diffs, 2.0))\n",
    "p_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3a8f31c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.4436)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_std = torch.pow(p_var, 0.5)\n",
    "p_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "545eb3c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6441)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phi_n = F.normalize(post_process.flatten(start_dim=1), dim=1)\n",
    "cos_sim = phi_n @ phi_n.T\n",
    "cos_sim.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f07947b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e360073",
   "metadata": {},
   "source": [
    "### Testing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4db25f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, test_loader, hyperparams, num_epochs, optimizer, loss_fn, output_file):\n",
    "    model.eval()\n",
    "    \n",
    "    total_loss = 0.0\n",
    "    all_labels = []\n",
    "    all_predictions = []\n",
    "    all_probs = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            \n",
    "            outputs = model(images)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            total_loss += loss.item(outputs, dim=1)[:, 1]\n",
    "            \n",
    "            probs = torch.softmax()\n",
    "            all_probs.extend(probs)\n",
    "            all_predictions.extend(outputs.argmax(dim=1).cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    accuracy = accuracy_score(all_labels, all_predictions)\n",
    "    precision = precision_score(all_labels, all_predictions, zero_division=0)\n",
    "    recall = recall_score(all_labels, all_predictions, zero_division=0)\n",
    "    f1 = f1_score(all_labels, all_predictions, zero_division=0)\n",
    "    try:\n",
    "        auc = roc_auc_score(all_labels, all_probs)\n",
    "    except ValueError:\n",
    "        auc = float(\"nan\")\n",
    "\n",
    "    avg_loss = total_loss / len(test_loader)\n",
    "    \n",
    "    with open(output_file, \"a\") as f:\n",
    "        f.write(f\"\\n[Quantum Model Testing - {time.perf_counter()}]\\n\")\n",
    "        f.write(f\"Hyperparameters: {hyperparams}\\n\")\n",
    "        f.write(f\"Test Loss: {avg_loss:.4f}\\n\")\n",
    "        f.write(f\"Accuracy: {accuracy:.4f}\\n\")\n",
    "        f.write(f\"Precision: {precision:.4f}\\n\")\n",
    "        f.write(f\"Recall: {recall:.4f}\\n\")\n",
    "        f.write(f\"F1 Score: {f1:.4f}\\n\")\n",
    "        f.write(f\"AUC: {auc:.4f}\\n\")\n",
    "        f.write(\"-\" * 50 + \"\\n\")\n",
    "\n",
    "    print(f\"Test Loss: {avg_loss:.4f}\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1: {f1:.4f}, AUC: {auc:.4f}\")\n",
    "\n",
    "    return avg_loss, accuracy, precision, recall, f1, auc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a319884f",
   "metadata": {},
   "source": [
    "### Validating model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d23f35ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_model(model, dataloader, criterion, device):\n",
    "    model.eval()    \n",
    "    model.to(device)\n",
    "\n",
    "    total_loss = 0.0\n",
    "    all_labels = []\n",
    "    all_predictions = []\n",
    "    all_probs = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(dataloader, desc=\"Validation\"):\n",
    "#            images = images.to(device)\n",
    "#            labels = labels.squeeze().to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            probs = torch.softmax(outputs, dim=1)[:, 1].cpu().numpy() if outputs.shape[1] > 1 else torch.softmax(outputs, dim=1)[:, 0].cpu().numpy()\n",
    "            all_probs.extend(probs)\n",
    "            all_predictions.extend(outputs.argmax(dim=1).cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    accuracy = accuracy_score(all_labels, all_predictions)\n",
    "    precision = precision_score(all_labels, all_predictions, zero_division=0)\n",
    "    recall = recall_score(all_labels, all_predictions, zero_division=0)\n",
    "    f1 = f1_score(all_labels, all_predictions, zero_division=0)\n",
    "    try:\n",
    "        auc = roc_auc_score(all_labels, all_probs)\n",
    "    except ValueError:\n",
    "        auc = float(\"nan\")\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    print(f\"Validation Loss: {avg_loss:.4f}\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1: {f1:.4f}, AUC: {auc:.4f}\")\n",
    "\n",
    "    return avg_loss, accuracy, precision, recall, f1, auc\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "58998e6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation:   0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passing through quanvolution layer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation:  25%|██▌       | 1/4 [00:04<00:14,  4.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quanvolution processing time: 4.72599090001313\n",
      "Passing through quanvolution layer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation:  50%|█████     | 2/4 [00:09<00:09,  4.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quanvolution processing time: 5.151228600007016\n",
      "Passing through quanvolution layer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation:  75%|███████▌  | 3/4 [00:15<00:05,  5.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quanvolution processing time: 5.20073370001046\n",
      "Passing through quanvolution layer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 4/4 [00:20<00:00,  5.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quanvolution processing time: 5.119025400024839\n",
      "Validation Loss: 0.5769\n",
      "Accuracy: 0.9450, Precision: 0.9045, Recall: 0.9950, F1: 0.9476, AUC: 0.9983\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.5768589973449707,\n",
       " 0.945,\n",
       " 0.9045454545454545,\n",
       " 0.995,\n",
       " 0.9476190476190476,\n",
       " 0.99835)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validate_model(hqcnn, validation_loader, loss_fn, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b83bc791",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 200 into shape (28,28)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[54]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m     12\u001b[39m images, labels = \u001b[38;5;28mnext\u001b[39m(dataiter)\n\u001b[32m     14\u001b[39m img_grid = make_grid(images)\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m \u001b[43mmatplotlib_imshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_grid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mone_channel\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[54]\u001b[39m\u001b[32m, line 5\u001b[39m, in \u001b[36mmatplotlib_imshow\u001b[39m\u001b[34m(img, one_channel)\u001b[39m\n\u001b[32m      3\u001b[39m     img = img.mean(dim=\u001b[32m0\u001b[39m)\n\u001b[32m      4\u001b[39m img = img / \u001b[32m2\u001b[39m + \u001b[32m0.5\u001b[39m     \u001b[38;5;66;03m# unnormalize\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m npimg = \u001b[43mimg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m28\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m28\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m one_channel:\n\u001b[32m      7\u001b[39m     plt.imshow(npimg, cmap=\u001b[33m\"\u001b[39m\u001b[33mGreys\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mValueError\u001b[39m: cannot reshape array of size 200 into shape (28,28)"
     ]
    }
   ],
   "source": [
    "def matplotlib_imshow(img, one_channel=False):\n",
    "    if one_channel:\n",
    "        img = img.mean(dim=0)\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy().reshape(28, 28)\n",
    "    if one_channel:\n",
    "        plt.imshow(npimg, cmap=\"Greys\")\n",
    "    else:\n",
    "        plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "\n",
    "dataiter = iter(training_loader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "img_grid = make_grid(images)\n",
    "matplotlib_imshow(img_grid, one_channel=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".genetic_circuits (3.12.10)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
