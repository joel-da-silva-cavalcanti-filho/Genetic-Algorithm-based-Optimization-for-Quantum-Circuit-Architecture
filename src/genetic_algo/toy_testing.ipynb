{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "59442a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ansatz_simulation_class import AnsatzSimulation\n",
    "from genetic_quantum import QuanvLayer, QuantumModel\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import make_grid\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch import tensor\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2caf39e",
   "metadata": {},
   "source": [
    "### Instantiating genetic ansatz model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "609bea41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QuantumModel(\n",
       "  (quanv_layer): QuanvLayer()\n",
       "  (fc1): Linear(in_features=2916, out_features=64, bias=True)\n",
       "  (fc2): Linear(in_features=64, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_qubits = 4\n",
    "patch_size = 2\n",
    "num_classes = 2\n",
    "input_size = 28\n",
    "toy_chromosome = [['ctrl_0', 'trgt_0', 'ctrl_1', 'trgt_1'], [None, 'ctrl_0', 'trgt_0', None], ['rz_gate', 'pauli_y', 'rx_gate', 'phase']]\n",
    "quanv = QuanvLayer(n_qubits, patch_size, toy_chromosome, mode='2d')\n",
    "toy_qhcnn = QuantumModel(n_qubits, num_classes, patch_size, input_size, toy_chromosome)\n",
    "toy_qhcnn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee926d7",
   "metadata": {},
   "source": [
    "### Getting data and sampling it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c98375c",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.ToTensor()\n",
    "mnist_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c9a0fee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12665"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_classes = [0, 1]\n",
    "reduced_mnist = [mnist_dataset.__getitem__(index) for index in range(mnist_dataset.__len__()) if mnist_dataset.__getitem__(index)[1] in selected_classes]\n",
    "len(reduced_mnist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d515ac3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_one = 0\n",
    "class_two = 0\n",
    "binary_dataset = []\n",
    "\n",
    "for image in reduced_mnist:\n",
    "    if image[1] == 0 and class_one < 50:\n",
    "        binary_dataset.append(image)\n",
    "        class_one +=1 \n",
    "    elif image[1] == 1 and class_two < 50:\n",
    "        binary_dataset.append(image)\n",
    "        class_two += 1\n",
    "        \n",
    "    if class_one == 50 and class_two == 50:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4dd9477a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_images, mnist_labels = zip(*binary_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "323a9509",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_mnist = torch.stack(mnist_images)\n",
    "height = tensor_mnist.shape[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e9bdc04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_labels = tensor(mnist_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f8b0aa8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 28, 28])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_mnist.squeeze(1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d2725fd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passing through quanvolution layer...\n",
      "Patch crafting time: 0.014747400302439928\n",
      "Image quantum processing time: 22.67309629963711\n",
      "Quanvolution processing time: 22.90649880003184\n",
      "Classical layers processing time: 0.0011980002745985985\n"
     ]
    }
   ],
   "source": [
    "output = toy_qhcnn.forward(tensor_mnist.squeeze(1).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "78d83385",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.5248, -0.8957],\n",
       "        [-0.5062, -0.9233],\n",
       "        [-0.5519, -0.8577],\n",
       "        [-0.5162, -0.9083],\n",
       "        [-0.5082, -0.9203],\n",
       "        [-0.5475, -0.8636],\n",
       "        [-0.5106, -0.9166],\n",
       "        [-0.5123, -0.9141],\n",
       "        [-0.5060, -0.9236],\n",
       "        [-0.4926, -0.9442],\n",
       "        [-0.4985, -0.9351],\n",
       "        [-0.5898, -0.8084],\n",
       "        [-0.6051, -0.7898],\n",
       "        [-0.5010, -0.9312],\n",
       "        [-0.5625, -0.8435],\n",
       "        [-0.5059, -0.9237],\n",
       "        [-0.5104, -0.9170],\n",
       "        [-0.5513, -0.8585],\n",
       "        [-0.5526, -0.8567],\n",
       "        [-0.5131, -0.9130],\n",
       "        [-0.5340, -0.8825],\n",
       "        [-0.5018, -0.9300],\n",
       "        [-0.5205, -0.9019],\n",
       "        [-0.5176, -0.9062],\n",
       "        [-0.5737, -0.8288],\n",
       "        [-0.5423, -0.8708],\n",
       "        [-0.5154, -0.9094],\n",
       "        [-0.5048, -0.9254],\n",
       "        [-0.5262, -0.8937],\n",
       "        [-0.5024, -0.9291],\n",
       "        [-0.4818, -0.9615],\n",
       "        [-0.5024, -0.9291],\n",
       "        [-0.4982, -0.9355],\n",
       "        [-0.5759, -0.8260],\n",
       "        [-0.5190, -0.9041],\n",
       "        [-0.5711, -0.8322],\n",
       "        [-0.5995, -0.7964],\n",
       "        [-0.5029, -0.9284],\n",
       "        [-0.5165, -0.9078],\n",
       "        [-0.5749, -0.8272],\n",
       "        [-0.4912, -0.9465],\n",
       "        [-0.5448, -0.8674],\n",
       "        [-0.5242, -0.8966],\n",
       "        [-0.4882, -0.9512],\n",
       "        [-0.5396, -0.8746],\n",
       "        [-0.5301, -0.8881],\n",
       "        [-0.5087, -0.9194],\n",
       "        [-0.5438, -0.8688],\n",
       "        [-0.5106, -0.9166],\n",
       "        [-0.4907, -0.9472],\n",
       "        [-0.6168, -0.7758],\n",
       "        [-0.5078, -0.9208],\n",
       "        [-0.6011, -0.7945],\n",
       "        [-0.5090, -0.9190],\n",
       "        [-0.5057, -0.9240],\n",
       "        [-0.5306, -0.8874],\n",
       "        [-0.5993, -0.7967],\n",
       "        [-0.6269, -0.7641],\n",
       "        [-0.4931, -0.9435],\n",
       "        [-0.5096, -0.9181],\n",
       "        [-0.5520, -0.8575],\n",
       "        [-0.5516, -0.8581],\n",
       "        [-0.5616, -0.8447],\n",
       "        [-0.5983, -0.7979],\n",
       "        [-0.5199, -0.9028],\n",
       "        [-0.5218, -0.9000],\n",
       "        [-0.4774, -0.9686],\n",
       "        [-0.4991, -0.9341],\n",
       "        [-0.4904, -0.9477],\n",
       "        [-0.5909, -0.8071],\n",
       "        [-0.4835, -0.9587],\n",
       "        [-0.5755, -0.8266],\n",
       "        [-0.5163, -0.9081],\n",
       "        [-0.5595, -0.8475],\n",
       "        [-0.4646, -0.9899],\n",
       "        [-0.4933, -0.9432],\n",
       "        [-0.5095, -0.9183],\n",
       "        [-0.5212, -0.9010],\n",
       "        [-0.5176, -0.9063],\n",
       "        [-0.5166, -0.9077],\n",
       "        [-0.5243, -0.8964],\n",
       "        [-0.4991, -0.9341],\n",
       "        [-0.5278, -0.8914],\n",
       "        [-0.5230, -0.8983],\n",
       "        [-0.5371, -0.8781],\n",
       "        [-0.5137, -0.9120],\n",
       "        [-0.5216, -0.9004],\n",
       "        [-0.5249, -0.8955],\n",
       "        [-0.5530, -0.8562],\n",
       "        [-0.5597, -0.8471],\n",
       "        [-0.6100, -0.7838],\n",
       "        [-0.5761, -0.8257],\n",
       "        [-0.5859, -0.8133],\n",
       "        [-0.5188, -0.9045],\n",
       "        [-0.5325, -0.8846],\n",
       "        [-0.5761, -0.8257],\n",
       "        [-0.5484, -0.8625],\n",
       "        [-0.5316, -0.8859],\n",
       "        [-0.5176, -0.9062],\n",
       "        [-0.5224, -0.8992]], grad_fn=<LogSoftmaxBackward0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c90f06",
   "metadata": {},
   "source": [
    "### Training my model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "def43503",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sampled_cv_dataset import SampledDataset4Training\n",
    "\n",
    "training_zeros_ones = SampledDataset4Training(binary_dataset)\n",
    "\n",
    "batch_size = 4\n",
    "\n",
    "training_loader = DataLoader(training_zeros_ones, batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ed5cfd0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 28, 28])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_zeros_ones.__getitem__(0)['image'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "970ae5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(toy_qhcnn.parameters(), lr=0.001, momentum=0.9)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "loss = loss_fn(output, mnist_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d7ca0b89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.7309, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a0bef67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, epoch_index, tb_writer):\n",
    "    running_loss = 0.\n",
    "    last_loss = 0.\n",
    "\n",
    "    # Here, we use enumerate(training_loader) instead of\n",
    "    # iter(training_loader) so that we can track the batch\n",
    "    # index and do some intra-epoch reporting\n",
    "    for i, data in enumerate(training_loader):\n",
    "        # Every data instance is an input + label pair\n",
    "        inputs, labels = data\n",
    "\n",
    "        # Zero your gradients for every batch!\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Make predictions for this batch\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Compute the loss and its gradients\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "\n",
    "        # Adjust learning weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # Gather data and report\n",
    "        running_loss += loss.item()\n",
    "        if i % 1000 == 999:\n",
    "            last_loss = running_loss / 1000 # loss per batch\n",
    "            print('  batch {} loss: {}'.format(i + 1, last_loss))\n",
    "            tb_x = epoch_index * len(training_loader) + i + 1\n",
    "            tb_writer.add_scalar('Loss/train', last_loss, tb_x)\n",
    "            running_loss = 0.\n",
    "\n",
    "    return last_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b83bc791",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "tensor or list of tensors expected, got <class 'str'>",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m     11\u001b[39m dataiter = \u001b[38;5;28miter\u001b[39m(training_loader)\n\u001b[32m     12\u001b[39m images, labels = \u001b[38;5;28mnext\u001b[39m(dataiter)\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m img_grid = \u001b[43mmake_grid\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m matplotlib_imshow(img_grid, one_channel=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\speak\\Genetic-Algorithm-based-Optimization-for-Quantum-Circuit-Synthesis\\src\\genetic_algo\\.genetic_circuits\\Lib\\site-packages\\torch\\utils\\_contextlib.py:120\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    119\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\speak\\Genetic-Algorithm-based-Optimization-for-Quantum-Circuit-Synthesis\\src\\genetic_algo\\.genetic_circuits\\Lib\\site-packages\\torchvision\\utils.py:64\u001b[39m, in \u001b[36mmake_grid\u001b[39m\u001b[34m(tensor, nrow, padding, normalize, value_range, scale_each, pad_value)\u001b[39m\n\u001b[32m     62\u001b[39m                 \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mtensor or list of tensors expected, got a list containing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(t)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     63\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m64\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mtensor or list of tensors expected, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(tensor)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     66\u001b[39m \u001b[38;5;66;03m# if list of tensors, convert to a 4D mini-batch Tensor\u001b[39;00m\n\u001b[32m     67\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tensor, \u001b[38;5;28mlist\u001b[39m):\n",
      "\u001b[31mTypeError\u001b[39m: tensor or list of tensors expected, got <class 'str'>"
     ]
    }
   ],
   "source": [
    "def matplotlib_imshow(img, one_channel=False):\n",
    "    if one_channel:\n",
    "        img = img.mean(dim=0)\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    if one_channel:\n",
    "        plt.imshow(npimg, cmap=\"Greys\")\n",
    "    else:\n",
    "        plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "\n",
    "dataiter = iter(training_loader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "img_grid = make_grid(images)\n",
    "matplotlib_imshow(img_grid, one_channel=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69bdd567",
   "metadata": {},
   "source": [
    "### Testing tensor dimensions and quantum convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "704233ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 3, 2, 2])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction import image\n",
    "from torch import tensor\n",
    "\n",
    "input_size = 4\n",
    "dummy_input = torch.zeros(input_size, input_size, 2).numpy()\n",
    "patched_images = [image.extract_patches_2d(img, (patch_size, patch_size)) for img in dummy_input]\n",
    "#dummy_output = quanv.forward(dummy_input)\n",
    "\n",
    "tensor(patched_images).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f186a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import image\n",
    "from torch import tensor\n",
    "\n",
    "input_size = 28\n",
    "shape = (input_size, input_size)\n",
    "dummy_input = np.zeros(shape, dtype=float)\n",
    "patched_images = image.extract_patches_2d(dummy_input, (patch_size, patch_size)) \n",
    "#dummy_output = quanv.forward(dummy_input)\n",
    "n_patches, patch_h, patch_w = tensor(patched_images).shape\n",
    "patched_images = tensor(patched_images).contiguous().view(n_patches, patch_h*patch_w)\n",
    "image._extract_patches()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a4f9a68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0.], dtype=torch.float64)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patched_images[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f9b26f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([729, 4])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patched_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9251b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "myAnsatz = AnsatzSimulation(n_qubits)\n",
    "outputs = [myAnsatz.simulate_circuit(patch, 'rx', toy_chromosome) for patch in patched_images]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523e60ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1458, 4])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor(outputs).shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".genetic_circuits (3.12.10)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
