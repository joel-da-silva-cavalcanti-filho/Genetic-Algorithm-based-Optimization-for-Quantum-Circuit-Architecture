{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "59442a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ansatz_simulation_class import AnsatzSimulation\n",
    "import torch\n",
    "from genetic_quantum import QuantumModel, QuanvLayer\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import make_grid\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "import pandas as pd\n",
    "from patch_making import PatchExtraction, Quanv_2d\n",
    "from torch import tensor\n",
    "import matplotlib.pyplot as plt\n",
    "from sampled_cv_dataset import SampledDataset4Training\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0efd0227",
   "metadata": {},
   "source": [
    "### Defining Classical Module class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9f0e2cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class L2NormalizationLayer(nn.Module):\n",
    "    def __init__(self, dim=1, eps=1e-12):\n",
    "        super(L2NormalizationLayer, self).__init__()\n",
    "        self.dim = dim\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.normalize(x, p=2, dim=self.dim, eps=self.eps)\n",
    "\n",
    "class HybridModel(nn.Module):\n",
    "    def __init__(self, n_qubits, patch_size, chromosome, num_classes, input_size, ansatz_parameters, mode='2d'):\n",
    "        super().__init__()\n",
    "        self.quanv_layer = QuanvLayer(\n",
    "            n_qubits=n_qubits,\n",
    "            patch_size=patch_size,\n",
    "            chromosome=chromosome,\n",
    "            parameters = ansatz_parameters,\n",
    "            mode=mode\n",
    "        )\n",
    "        feature_size = input_size\n",
    "        self.fc1 = nn.Linear(feature_size, 64)\n",
    "        self.fc2 = nn.Linear(64, num_classes)\n",
    "        self.norm = nn.LayerNorm(feature_size)\n",
    "        #self.conv1 = nn.Conv2d(in_channels = 1, output_channels=16, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=patch_size, stride=patch_size)\n",
    "        self.fc = nn.Linear(feature_size, num_classes)\n",
    "        self.l2norm = L2NormalizationLayer(dim=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        print(\"Passing through quanvolution layer...\")\n",
    "        start_time = time.perf_counter()\n",
    "        x = self.quanv_layer.forward(x)\n",
    "        end_time = time.perf_counter()\n",
    "        print(f'Quanvolution processing time: {end_time - start_time}')  \n",
    "        x = x.flatten(start_dim=1)\n",
    "        #print(x.shape)\n",
    "        x = self.l2norm(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2caf39e",
   "metadata": {},
   "source": [
    "### Instantiating genetic ansatz model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "609bea41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HybridModel(\n",
       "  (quanv_layer): QuanvLayer()\n",
       "  (fc1): Linear(in_features=320, out_features=64, bias=True)\n",
       "  (fc2): Linear(in_features=64, out_features=2, bias=True)\n",
       "  (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (fc): Linear(in_features=320, out_features=2, bias=True)\n",
       "  (l2norm): L2NormalizationLayer()\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_qubits = 4\n",
    "max_patches = 80\n",
    "patch_size = 2\n",
    "batch_size = 100\n",
    "num_classes = 2\n",
    "input_size = max_patches * n_qubits\n",
    "num_epochs = 4\n",
    "toy_chromosome = [['ctrl_0', 'trgt_0', 'ctrl_1', 'trgt_1'], [None, 'ctrl_0', 'trgt_0', None], ['ry_gate', 'rz_gate', 'ry_gate', 'rx_gate'],['trgt_0', 'ctrl_0', 'trgt_1', 'ctrl_1'], ['rz_gate', 'trgt_0', 'ctrl_0', 'ry_gate']]\n",
    "#simple_fc = ClassicalComponent(num_classes, input_size)\n",
    "params = torch.rand(6)*math.pi\n",
    "hqcnn = HybridModel(n_qubits, patch_size, toy_chromosome, num_classes, input_size, params)\n",
    "hqcnn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee926d7",
   "metadata": {},
   "source": [
    "### Getting data and sampling it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "da7dfc49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_classes(target_classes, dataset):\n",
    "    return [dataset.__getitem__(index) for index in range(dataset.__len__()) if dataset.__getitem__(index)[1] in target_classes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f017c858",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_data(dataset, classes, sample_size):\n",
    "    class_one = 0\n",
    "    class_two = 0\n",
    "    binary_dataset = []\n",
    "\n",
    "    for image in dataset:\n",
    "        if image[1] == classes[0] and class_one < sample_size:\n",
    "            binary_dataset.append(image)\n",
    "            class_one +=1 \n",
    "        elif image[1] == [1] and class_two < sample_size:\n",
    "            binary_dataset.append(image)\n",
    "            class_two += 1\n",
    "            \n",
    "        if class_one == sample_size and class_two == sample_size:\n",
    "            break\n",
    "    \n",
    "    return binary_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0dbbd103",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(dataset_class, transform_routine, target_classes, sample_size):\n",
    "    training_dataset = dataset_class(root='./data', train=True, download=True, transform=transform_routine)\n",
    "    validation_dataset = dataset_class(root='./data', train=False, transform=transform_routine, download=True)\n",
    "    \n",
    "    reduced_training_dataset = sample_classes(target_classes, training_dataset)\n",
    "    reduced_validation_dataset = sample_classes(target_classes, validation_dataset)\n",
    "    \n",
    "    sampled_training_dataset = sample_data(reduced_training_dataset, target_classes, sample_size)\n",
    "    sampled_validation_dataset = sample_data(reduced_validation_dataset, target_classes, sample_size)\n",
    "    \n",
    "    return SampledDataset4Training(sampled_training_dataset), SampledDataset4Training(sampled_validation_dataset) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "61e937b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    PatchExtraction(patch_size, max_patches)])\n",
    "\n",
    "target_classes = [0, 1]\n",
    "\n",
    "sample_size = 200\n",
    "\n",
    "training_data, validation_data = create_dataset(datasets.MNIST, transform, target_classes, sample_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8c98375c",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    PatchExtraction(patch_size, max_patches)])\n",
    "mnist_dataset = datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c282b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_validation_set = datasets.FashionMNIST(root='./data', train=False, transform=transform, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bae8129d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.9373, 0.9647, 0.9569, 0.8627],\n",
       "         [0.8745, 1.0000, 0.9176, 0.8275],\n",
       "         [0.8941, 0.9098, 0.8549, 0.9176],\n",
       "         [0.8745, 0.9216, 0.8784, 0.8784],\n",
       "         [0.8078, 1.0000, 0.8667, 0.8667],\n",
       "         [0.9020, 0.9176, 0.7373, 0.9725],\n",
       "         [0.9608, 0.8745, 0.8314, 0.8706],\n",
       "         [0.9490, 0.9529, 0.8667, 0.7569],\n",
       "         [0.9294, 0.8510, 0.8706, 0.8706],\n",
       "         [0.8980, 0.9176, 0.8627, 0.8431],\n",
       "         [0.8941, 0.8902, 0.8706, 0.8667],\n",
       "         [0.8706, 0.8980, 0.9765, 0.7608],\n",
       "         [0.8902, 0.9373, 0.8549, 0.8196],\n",
       "         [0.9412, 0.8353, 0.8745, 0.8510],\n",
       "         [0.8667, 0.8510, 0.8745, 0.8980],\n",
       "         [0.8863, 0.8745, 0.8588, 0.8667],\n",
       "         [0.8941, 0.8706, 0.8863, 0.8039],\n",
       "         [0.8941, 0.8353, 0.8549, 0.8275],\n",
       "         [0.8157, 0.8784, 0.8863, 0.8196],\n",
       "         [0.8275, 0.8784, 0.6902, 0.9804],\n",
       "         [0.7529, 0.8392, 0.8667, 0.9255],\n",
       "         [0.8000, 0.8157, 0.7843, 0.9608],\n",
       "         [0.7961, 0.8667, 0.8353, 0.8627],\n",
       "         [0.8118, 0.8667, 0.8157, 0.8549],\n",
       "         [0.8471, 0.7725, 0.7765, 0.9412],\n",
       "         [0.8549, 0.7765, 0.8431, 0.8706],\n",
       "         [0.8588, 0.8314, 0.7529, 0.8902],\n",
       "         [0.8275, 0.8392, 0.8039, 0.8627],\n",
       "         [0.8235, 0.8353, 0.8627, 0.7922],\n",
       "         [0.7490, 0.8000, 0.8706, 0.8824],\n",
       "         [0.8667, 0.8275, 0.8039, 0.8039],\n",
       "         [0.9137, 0.8745, 0.8431, 0.6431],\n",
       "         [0.8000, 0.8000, 0.8039, 0.8824],\n",
       "         [0.7608, 0.7922, 0.8588, 0.8627],\n",
       "         [0.8784, 0.8588, 0.8784, 0.6235],\n",
       "         [0.7412, 0.8627, 0.8510, 0.7843],\n",
       "         [0.7569, 0.8549, 0.7765, 0.8314],\n",
       "         [0.9412, 0.5882, 0.8980, 0.7373],\n",
       "         [0.8980, 0.8745, 0.8353, 0.4980],\n",
       "         [0.7373, 0.7765, 0.8196, 0.8235],\n",
       "         [0.8745, 0.8431, 1.0000, 0.0000],\n",
       "         [0.8275, 0.7843, 0.7608, 0.7647],\n",
       "         [0.7255, 0.8078, 0.8353, 0.7647],\n",
       "         [0.7412, 0.7569, 0.8000, 0.8235],\n",
       "         [0.8627, 0.9647, 0.3843, 0.7765],\n",
       "         [0.8824, 0.7804, 0.7294, 0.6941],\n",
       "         [0.8745, 0.8431, 0.9569, 0.0000],\n",
       "         [0.8235, 0.7686, 0.7490, 0.7490],\n",
       "         [0.6706, 0.7686, 0.8235, 0.8118],\n",
       "         [0.7961, 0.7608, 0.7725, 0.7451],\n",
       "         [0.8275, 0.8745, 0.9529, 0.0000],\n",
       "         [0.8471, 0.8980, 0.8549, 0.3020],\n",
       "         [0.7020, 0.7176, 0.8000, 0.8353],\n",
       "         [0.9137, 0.8431, 0.0118, 0.8941],\n",
       "         [0.9098, 0.0000, 0.9137, 0.8235],\n",
       "         [0.8392, 0.8196, 0.6235, 0.7569],\n",
       "         [0.8471, 0.8078, 0.7098, 0.6745],\n",
       "         [0.8627, 0.9020, 0.0000, 0.8588],\n",
       "         [0.9333, 0.0000, 0.7922, 0.8784],\n",
       "         [0.8431, 0.8745, 0.8784, 0.1137],\n",
       "         [0.8275, 0.7373, 0.7529, 0.6667],\n",
       "         [0.3137, 1.0000, 0.8667, 0.6039],\n",
       "         [0.7765, 0.6902, 0.6549, 0.8235],\n",
       "         [0.8314, 0.6863, 0.6863, 0.7255],\n",
       "         [0.8235, 0.8196, 0.8941, 0.0000],\n",
       "         [0.2235, 0.8157, 0.8667, 0.8157],\n",
       "         [0.0000, 0.8118, 0.8549, 0.8471],\n",
       "         [0.0000, 0.7333, 0.8784, 0.8784],\n",
       "         [0.7490, 0.6745, 0.7098, 0.7373],\n",
       "         [0.8549, 0.8314, 0.7725, 0.2039],\n",
       "         [0.2941, 0.8314, 0.7569, 0.8275],\n",
       "         [0.7490, 0.7137, 0.7098, 0.6510],\n",
       "         [0.0000, 0.8941, 0.8353, 0.7059],\n",
       "         [0.2706, 0.8745, 0.8471, 0.6392],\n",
       "         [0.9451, 0.2863, 0.4588, 0.8588],\n",
       "         [0.0471, 0.8627, 0.8549, 0.6627],\n",
       "         [0.7020, 0.7137, 0.6902, 0.6588],\n",
       "         [0.7373, 0.7608, 0.8471, 0.0000],\n",
       "         [0.1882, 0.7176, 0.8353, 0.7255],\n",
       "         [0.0000, 0.9255, 0.6980, 0.6118]]),\n",
       " 9)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist_dataset.__getitem__(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9208afbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mnist_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7c9a0fee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12000"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_classes = [1, 4]\n",
    "reduced_mnist = [mnist_dataset.__getitem__(index) for index in range(mnist_dataset.__len__()) if mnist_dataset.__getitem__(index)[1] in selected_classes]\n",
    "len(reduced_mnist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fe6c7305",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.0000, 0.9059, 0.9922, 0.9882],\n",
       "         [0.9882, 0.8902, 0.9059, 0.0000],\n",
       "         [0.7725, 0.9922, 0.9843, 0.0000],\n",
       "         [0.9922, 0.5412, 0.0941, 0.9882],\n",
       "         [0.9216, 0.8510, 0.1647, 0.7529],\n",
       "         [0.8235, 0.9882, 0.6588, 0.0000],\n",
       "         [0.9451, 0.9882, 0.3020, 0.0000],\n",
       "         [0.9882, 0.9922, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0627, 0.9882, 0.9882],\n",
       "         [0.5804, 0.9922, 0.4431, 0.5804],\n",
       "         [0.2157, 0.8235, 0.9922, 0.3412],\n",
       "         [0.0000, 0.0000, 0.9098, 0.9922],\n",
       "         [0.8863, 0.9882, 0.0000, 0.0000],\n",
       "         [0.0549, 0.0000, 0.9882, 0.8824],\n",
       "         [0.0549, 0.0000, 0.8431, 0.9882],\n",
       "         [0.0000, 0.3412, 0.9882, 0.7412],\n",
       "         [0.7176, 0.0000, 0.3608, 0.9882],\n",
       "         [0.9882, 0.0000, 0.0000, 0.7882],\n",
       "         [0.7137, 0.9922, 0.0000, 0.0000],\n",
       "         [0.6902, 0.0000, 0.1412, 0.9882],\n",
       "         [0.9922, 0.6902, 0.0000, 0.0314],\n",
       "         [0.0157, 0.9490, 0.7451, 0.0196],\n",
       "         [0.0000, 0.0000, 0.9922, 0.5725],\n",
       "         [0.0000, 0.0000, 0.5176, 0.9882],\n",
       "         [0.0000, 0.9882, 0.1176, 0.4667],\n",
       "         [0.0471, 0.9882, 0.4549, 0.0000],\n",
       "         [0.9608, 0.5059, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.4157, 0.9882],\n",
       "         [0.0000, 0.0000, 0.3765, 0.9882],\n",
       "         [0.0000, 0.2235, 0.9882, 0.2549],\n",
       "         [0.0000, 0.0000, 0.9882, 0.3098],\n",
       "         [0.2784, 0.9882, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.2784, 0.9882],\n",
       "         [0.0000, 0.0000, 0.2431, 0.9922],\n",
       "         [0.9686, 0.0353, 0.0000, 0.3059],\n",
       "         [0.1765, 0.9922, 0.0000, 0.0000],\n",
       "         [0.0000, 1.0000, 0.0824, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 1.0000],\n",
       "         [0.0000, 0.0863, 0.9882, 0.0863],\n",
       "         [0.0000, 0.9922, 0.0824, 0.0000],\n",
       "         [0.0000, 0.9922, 0.0824, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.9922],\n",
       "         [0.0000, 0.0000, 0.0000, 0.9922],\n",
       "         [0.9922, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0549, 0.9882, 0.0000],\n",
       "         [0.9882, 0.0431, 0.0000, 0.0000],\n",
       "         [0.9882, 0.0431, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.9882],\n",
       "         [0.0000, 0.0000, 0.0000, 0.9882],\n",
       "         [0.0000, 0.9882, 0.0000, 0.0000]]),\n",
       " 9)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist_dataset.__getitem__(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d515ac3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class_one = 0\n",
    "class_two = 0\n",
    "binary_dataset = []\n",
    "\n",
    "for image in reduced_mnist:\n",
    "    if image[1] == 0 and class_one < 200:\n",
    "        binary_dataset.append(image)\n",
    "        class_one +=1 \n",
    "    elif image[1] == 1 and class_two < 200:\n",
    "        binary_dataset.append(image)\n",
    "        class_two += 1\n",
    "        \n",
    "    if class_one == 200 and class_two == 200:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6dcef0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels = zip(*binary_dataset)\n",
    "mnist_images = torch.stack(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "02ee1992",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([400, 50, 4])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5b3ae5e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.4163)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean = torch.mean(mnist_images)\n",
    "mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a7b5434b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1968)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diffs = mnist_images - mean\n",
    "var = torch.mean(torch.pow(diffs, 2.0))\n",
    "var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fbe13f0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.4436)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "std = torch.pow(var, 0.5)\n",
    "std"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c90f06",
   "metadata": {},
   "source": [
    "### Training my model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3c2e6dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#quantum_processing = transforms.Compose([Quanv_2d(n_qubits, toy_chromosome)])\n",
    "#training_zeros_ones = SampledDataset4Training(binary_dataset)\n",
    "training_loader = DataLoader(training_data, batch_size, shuffle=True)\n",
    "validation_loader = DataLoader(validation_data, batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "970ae5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(hqcnn.parameters(), lr=0.1, weight_decay=1e-4)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "#loss = loss_fn(output, mnist_labels)\n",
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f2fc876a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: tensor(0.4046)\n",
      "std: tensor(0.4424)\n"
     ]
    }
   ],
   "source": [
    "mean = 0.\n",
    "meansq = 0.\n",
    "for (data, label) in training_loader:\n",
    "    mean = data.mean()\n",
    "    meansq = (data**2).mean()\n",
    "\n",
    "std = torch.sqrt(meansq - mean**2)\n",
    "print(\"mean: \" + str(mean))\n",
    "print(\"std: \" + str(std))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "08f4aba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "post_processing = [data for (data, label) in training_loader]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8276ca39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 100, 50, 4])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "post_process = torch.stack(post_processing).squeeze(0)\n",
    "post_process.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3fe35fff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.4163)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_p = torch.mean(post_process)\n",
    "mean_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2bfbc050",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1968)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_diffs = post_process - mean_p\n",
    "p_var = torch.mean(torch.pow(p_diffs, 2.0))\n",
    "p_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3a8f31c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.4436)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_std = torch.pow(p_var, 0.5)\n",
    "p_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "545eb3c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6441)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phi_n = F.normalize(post_process.flatten(start_dim=1), dim=1)\n",
    "cos_sim = phi_n @ phi_n.T\n",
    "cos_sim.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01040e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, num_epochs, optimizer, loss_fn, filepath):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  # Set model to training mode\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        epoch_loss = 0.0\n",
    "        total = 0\n",
    "        \n",
    "        progress_bar = tqdm(enumerate(train_loader), total=len(train_loader), desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        for inputs, labels in progress_bar:\n",
    "        \n",
    "            optimizer.zero_grad()  # Zero out previous gradients\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_fn(outputs, labels)  # Calculate loss\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "            \n",
    "            loss.backward()  # Backpropagate to calculate gradients\n",
    "            optimizer.step() # Update weights\n",
    "            \n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_loader)}], Loss: {running_loss/(i+1):.4f}, Acc: {correct / total:.4f}\")\n",
    "            \n",
    "        epoch_loss = running_loss / len(train_loader)\n",
    "        epoch_acc = correct / total\n",
    "        print(f\"Epoch {epoch+1}: Train Loss={epoch_loss:.4f}, Train Acc={epoch_acc:.4f}\")\n",
    "            # Print every 10 batches\n",
    "        torch.save(model.state_dict(), filepath)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f07947b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db25f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, test_loader, hyperparams, num_epochs, optimizer, loss_fn, output_file):\n",
    "    model.eval()\n",
    "    \n",
    "    total_loss = 0.0\n",
    "    all_labels = []\n",
    "    all_predictions = []\n",
    "    all_probs = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            \n",
    "            outputs = model(images)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            total_loss += loss.item(outputs, dim=1)[:, 1]\n",
    "            \n",
    "            probs = torch.softmax()\n",
    "            all_probs.extend(probs)\n",
    "            all_predictions.extend(outputs.argmax(dim=1).cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    accuracy = accuracy_score(all_labels, all_predictions)\n",
    "    precision = precision_score(all_labels, all_predictions, zero_division=0)\n",
    "    recall = recall_score(all_labels, all_predictions, zero_division=0)\n",
    "    f1 = f1_score(all_labels, all_predictions, zero_division=0)\n",
    "    try:\n",
    "        auc = roc_auc_score(all_labels, all_probs)\n",
    "    except ValueError:\n",
    "        auc = float(\"nan\")\n",
    "\n",
    "    avg_loss = total_loss / len(test_loader)\n",
    "    \n",
    "    with open(output_file, \"a\") as f:\n",
    "        f.write(f\"\\n[Quantum Model Testing - {time.perf_counter()}]\\n\")\n",
    "        f.write(f\"Hyperparameters: {hyperparams}\\n\")\n",
    "        f.write(f\"Test Loss: {avg_loss:.4f}\\n\")\n",
    "        f.write(f\"Accuracy: {accuracy:.4f}\\n\")\n",
    "        f.write(f\"Precision: {precision:.4f}\\n\")\n",
    "        f.write(f\"Recall: {recall:.4f}\\n\")\n",
    "        f.write(f\"F1 Score: {f1:.4f}\\n\")\n",
    "        f.write(f\"AUC: {auc:.4f}\\n\")\n",
    "        f.write(\"-\" * 50 + \"\\n\")\n",
    "\n",
    "    print(f\"Test Loss: {avg_loss:.4f}\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1: {f1:.4f}, AUC: {auc:.4f}\")\n",
    "\n",
    "    return avg_loss, accuracy, precision, recall, f1, auc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23f35ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_model(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    total_loss = 0.0\n",
    "    all_labels = []\n",
    "    all_predictions = []\n",
    "    all_probs = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(dataloader, desc=\"Validation\"):\n",
    "#            images = images.to(device)\n",
    "#            labels = labels.squeeze().to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            probs = torch.softmax(outputs, dim=1)[:, 1].cpu().numpy() if outputs.shape[1] > 1 else torch.softmax(outputs, dim=1)[:, 0].cpu().numpy()\n",
    "            all_probs.extend(probs)\n",
    "            all_predictions.extend(outputs.argmax(dim=1).cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    accuracy = accuracy_score(all_labels, all_predictions)\n",
    "    precision = precision_score(all_labels, all_predictions, zero_division=0)\n",
    "    recall = recall_score(all_labels, all_predictions, zero_division=0)\n",
    "    f1 = f1_score(all_labels, all_predictions, zero_division=0)\n",
    "    try:\n",
    "        auc = roc_auc_score(all_labels, all_probs)\n",
    "    except ValueError:\n",
    "        auc = float(\"nan\")\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    print(f\"Validation Loss: {avg_loss:.4f}\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1: {f1:.4f}, AUC: {auc:.4f}\")\n",
    "\n",
    "    return avg_loss, accuracy, precision, recall, f1, auc\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10146a35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passing through quanvolution layer...\n",
      "Quanvolution processing time: 2.356858999992255\n",
      "Epoch [1/4], Step [1/4], Loss: 0.7029, Acc: 0.4800\n",
      "Passing through quanvolution layer...\n",
      "Quanvolution processing time: 2.103883000003407\n",
      "Epoch [1/4], Step [2/4], Loss: 0.7010, Acc: 0.4850\n",
      "Passing through quanvolution layer...\n",
      "Quanvolution processing time: 2.0533047999924747\n",
      "Epoch [1/4], Step [3/4], Loss: 0.6972, Acc: 0.5000\n",
      "Passing through quanvolution layer...\n",
      "Quanvolution processing time: 2.0809785000019474\n",
      "Epoch [1/4], Step [4/4], Loss: 0.6976, Acc: 0.5000\n",
      "Epoch 1: Train Loss=0.6976, Train Acc=0.5000\n",
      "Passing through quanvolution layer...\n",
      "Quanvolution processing time: 2.122514400005457\n",
      "Epoch [2/4], Step [1/4], Loss: 0.7043, Acc: 0.4600\n",
      "Passing through quanvolution layer...\n",
      "Quanvolution processing time: 2.2003954000101658\n",
      "Epoch [2/4], Step [2/4], Loss: 0.6977, Acc: 0.4900\n",
      "Passing through quanvolution layer...\n",
      "Quanvolution processing time: 2.687404600001173\n",
      "Epoch [2/4], Step [3/4], Loss: 0.6979, Acc: 0.5000\n",
      "Passing through quanvolution layer...\n",
      "Quanvolution processing time: 2.397178500003065\n",
      "Epoch [2/4], Step [4/4], Loss: 0.6976, Acc: 0.5000\n",
      "Epoch 2: Train Loss=0.6976, Train Acc=0.5000\n",
      "Passing through quanvolution layer...\n",
      "Quanvolution processing time: 2.236737600003835\n",
      "Epoch [3/4], Step [1/4], Loss: 0.7139, Acc: 0.4100\n",
      "Passing through quanvolution layer...\n",
      "Quanvolution processing time: 2.281245800011675\n",
      "Epoch [3/4], Step [2/4], Loss: 0.7019, Acc: 0.4700\n",
      "Passing through quanvolution layer...\n",
      "Quanvolution processing time: 2.3876097000029404\n",
      "Epoch [3/4], Step [3/4], Loss: 0.6980, Acc: 0.4933\n",
      "Passing through quanvolution layer...\n",
      "Quanvolution processing time: 2.1581999999907566\n",
      "Epoch [3/4], Step [4/4], Loss: 0.6976, Acc: 0.5000\n",
      "Epoch 3: Train Loss=0.6976, Train Acc=0.5000\n",
      "Passing through quanvolution layer...\n",
      "Quanvolution processing time: 2.400881199995638\n",
      "Epoch [4/4], Step [1/4], Loss: 0.6944, Acc: 0.5200\n",
      "Passing through quanvolution layer...\n",
      "Quanvolution processing time: 2.300664800000959\n",
      "Epoch [4/4], Step [2/4], Loss: 0.6990, Acc: 0.4950\n",
      "Passing through quanvolution layer...\n",
      "Quanvolution processing time: 2.1583725000091363\n",
      "Epoch [4/4], Step [3/4], Loss: 0.6998, Acc: 0.4867\n",
      "Passing through quanvolution layer...\n",
      "Quanvolution processing time: 2.4282603000028757\n",
      "Epoch [4/4], Step [4/4], Loss: 0.6976, Acc: 0.5000\n",
      "Epoch 4: Train Loss=0.6976, Train Acc=0.5000\n"
     ]
    }
   ],
   "source": [
    "#simple_fc = ClassicalComponent(num_classes, input_size)\n",
    "filepath = \"C:\\Users\\speak\\Genetic-Algorithm-based-Optimization-for-Quantum-Circuit-Synthesis\\src\\models\\hqcnn_weights.pth\"\n",
    "\n",
    "train_model(hqcnn, training_loader, num_epochs, optimizer, loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b83bc791",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 200 into shape (28,28)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[54]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m     12\u001b[39m images, labels = \u001b[38;5;28mnext\u001b[39m(dataiter)\n\u001b[32m     14\u001b[39m img_grid = make_grid(images)\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m \u001b[43mmatplotlib_imshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_grid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mone_channel\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[54]\u001b[39m\u001b[32m, line 5\u001b[39m, in \u001b[36mmatplotlib_imshow\u001b[39m\u001b[34m(img, one_channel)\u001b[39m\n\u001b[32m      3\u001b[39m     img = img.mean(dim=\u001b[32m0\u001b[39m)\n\u001b[32m      4\u001b[39m img = img / \u001b[32m2\u001b[39m + \u001b[32m0.5\u001b[39m     \u001b[38;5;66;03m# unnormalize\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m npimg = \u001b[43mimg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m28\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m28\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m one_channel:\n\u001b[32m      7\u001b[39m     plt.imshow(npimg, cmap=\u001b[33m\"\u001b[39m\u001b[33mGreys\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mValueError\u001b[39m: cannot reshape array of size 200 into shape (28,28)"
     ]
    }
   ],
   "source": [
    "def matplotlib_imshow(img, one_channel=False):\n",
    "    if one_channel:\n",
    "        img = img.mean(dim=0)\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy().reshape(28, 28)\n",
    "    if one_channel:\n",
    "        plt.imshow(npimg, cmap=\"Greys\")\n",
    "    else:\n",
    "        plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "\n",
    "dataiter = iter(training_loader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "img_grid = make_grid(images)\n",
    "matplotlib_imshow(img_grid, one_channel=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69bdd567",
   "metadata": {},
   "source": [
    "### Testing tensor dimensions and quantum convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "704233ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 3, 2, 2])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction import image\n",
    "from torch import tensor\n",
    "\n",
    "input_size = 4\n",
    "dummy_input = torch.zeros(input_size, input_size, 2).numpy()\n",
    "patched_images = [image.extract_patches_2d(img, (patch_size, patch_size)) for img in dummy_input]\n",
    "#dummy_output = quanv.forward(dummy_input)\n",
    "\n",
    "tensor(patched_images).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f186a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import image\n",
    "from torch import tensor\n",
    "\n",
    "input_size = 28\n",
    "\n",
    "shape = (input_size, input_size)\n",
    "dummy_input = np.zeros(shape, dtype=float)\n",
    "patched_images = image.extract_patches_2d(dummy_input, (patch_size, patch_size)) \n",
    "#dummy_output = quanv.forward(dummy_input)\n",
    "n_patches, patch_h, patch_w = tensor(patched_images).shape\n",
    "patched_images = tensor(patched_images).contiguous().view(n_patches, patch_h*patch_w)\n",
    "image._extract_patches()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a4f9a68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0.], dtype=torch.float64)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patched_images[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f9b26f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([729, 4])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patched_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9251b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "myAnsatz = AnsatzSimulation(n_qubits)\n",
    "outputs = [myAnsatz.simulate_circuit(patch, 'rx', toy_chromosome) for patch in patched_images]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523e60ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1458, 4])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor(outputs).shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".genetic_circuits (3.12.10)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
