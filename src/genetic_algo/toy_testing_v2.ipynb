{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "59442a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ansatz_simulation_class import AnsatzSimulation\n",
    "import torch\n",
    "from genetic_quantum import QuantumModel, QuanvLayer\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import make_grid\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from patch_making import PatchExtraction, Quanv_2d\n",
    "from torch import tensor\n",
    "import matplotlib.pyplot as plt\n",
    "from sampled_cv_dataset import SampledDataset4Training\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0efd0227",
   "metadata": {},
   "source": [
    "### Defining Classical Module class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "b9f0e2cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class L2NormalizationLayer(nn.Module):\n",
    "    def __init__(self, dim=1, eps=1e-12):\n",
    "        super(L2NormalizationLayer, self).__init__()\n",
    "        self.dim = dim\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.normalize(x, p=2, dim=self.dim, eps=self.eps)\n",
    "\n",
    "class HybridModel(nn.Module):\n",
    "    def __init__(self, n_qubits, patch_size, chromosome, num_classes, input_size, mode='2d',):\n",
    "        super().__init__()\n",
    "        self.quanv_layer = QuanvLayer(\n",
    "            n_qubits=n_qubits,\n",
    "            patch_size=patch_size,\n",
    "            chromosome=chromosome,\n",
    "            mode=mode\n",
    "        )\n",
    "        feature_size = input_size\n",
    "        self.fc1 = nn.Linear(feature_size, 64)\n",
    "        self.fc2 = nn.Linear(64, num_classes)\n",
    "        self.norm = nn.LayerNorm(feature_size)\n",
    "        self.fc = nn.Linear(feature_size, num_classes)\n",
    "        self.l2norm = L2NormalizationLayer(dim=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        print(\"Passing through quanvolution layer...\")\n",
    "        start_time = time.perf_counter()\n",
    "        x = self.quanv_layer.forward(x)\n",
    "        end_time =   time.perf_counter()\n",
    "        print(f'Quanvolution processing time: {end_time - start_time}')  \n",
    "        x = x.flatten(start_dim=1)\n",
    "        #print(x.shape)\n",
    "        #x = self.l2norm(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2caf39e",
   "metadata": {},
   "source": [
    "### Instantiating genetic ansatz model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "609bea41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HybridModel(\n",
       "  (quanv_layer): QuanvLayer()\n",
       "  (fc1): Linear(in_features=200, out_features=64, bias=True)\n",
       "  (fc2): Linear(in_features=64, out_features=2, bias=True)\n",
       "  (norm): LayerNorm((200,), eps=1e-05, elementwise_affine=True)\n",
       "  (fc): Linear(in_features=200, out_features=2, bias=True)\n",
       "  (l2norm): L2NormalizationLayer()\n",
       ")"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_qubits = 4\n",
    "max_patches = 50\n",
    "patch_size = 2\n",
    "batch_size = 100\n",
    "num_classes = 2\n",
    "input_size = max_patches * n_qubits\n",
    "num_epochs = 4\n",
    "toy_chromosome = [['ctrl_0', 'trgt_0', 'ctrl_1', 'trgt_1'], [None, 'ctrl_0', 'trgt_0', None], ['ry_gate', 'rz_gate', 'ry_gate', 'rx_gate'],['trgt_0', 'ctrl_0', 'trgt_1', 'ctrl_1'], ['rz_gate', 'trgt_0', 'ctrl_0', 'ry_gate']]\n",
    "#simple_fc = ClassicalComponent(num_classes, input_size)\n",
    "hqcnn = HybridModel(n_qubits, patch_size, toy_chromosome, num_classes, input_size)\n",
    "hqcnn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee926d7",
   "metadata": {},
   "source": [
    "### Getting data and sampling it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8c98375c",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    PatchExtraction(patch_size, max_patches)])\n",
    "mnist_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9208afbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mnist_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7c9a0fee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12665"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_classes = [0, 1]\n",
    "reduced_mnist = [mnist_dataset.__getitem__(index) for index in range(mnist_dataset.__len__()) if mnist_dataset.__getitem__(index)[1] in selected_classes]\n",
    "len(reduced_mnist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fe6c7305",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.0000, 0.9059, 0.9922, 0.9882],\n",
       "         [0.9882, 0.8902, 0.9059, 0.0000],\n",
       "         [0.7725, 0.9922, 0.9843, 0.0000],\n",
       "         [0.9922, 0.5412, 0.0941, 0.9882],\n",
       "         [0.9216, 0.8510, 0.1647, 0.7529],\n",
       "         [0.8235, 0.9882, 0.6588, 0.0000],\n",
       "         [0.9451, 0.9882, 0.3020, 0.0000],\n",
       "         [0.9882, 0.9922, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0627, 0.9882, 0.9882],\n",
       "         [0.5804, 0.9922, 0.4431, 0.5804],\n",
       "         [0.2157, 0.8235, 0.9922, 0.3412],\n",
       "         [0.0000, 0.0000, 0.9098, 0.9922],\n",
       "         [0.8863, 0.9882, 0.0000, 0.0000],\n",
       "         [0.0549, 0.0000, 0.9882, 0.8824],\n",
       "         [0.0549, 0.0000, 0.8431, 0.9882],\n",
       "         [0.0000, 0.3412, 0.9882, 0.7412],\n",
       "         [0.7176, 0.0000, 0.3608, 0.9882],\n",
       "         [0.9882, 0.0000, 0.0000, 0.7882],\n",
       "         [0.7137, 0.9922, 0.0000, 0.0000],\n",
       "         [0.6902, 0.0000, 0.1412, 0.9882],\n",
       "         [0.9922, 0.6902, 0.0000, 0.0314],\n",
       "         [0.0157, 0.9490, 0.7451, 0.0196],\n",
       "         [0.0000, 0.0000, 0.9922, 0.5725],\n",
       "         [0.0000, 0.0000, 0.5176, 0.9882],\n",
       "         [0.0000, 0.9882, 0.1176, 0.4667],\n",
       "         [0.0471, 0.9882, 0.4549, 0.0000],\n",
       "         [0.9608, 0.5059, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.4157, 0.9882],\n",
       "         [0.0000, 0.0000, 0.3765, 0.9882],\n",
       "         [0.0000, 0.2235, 0.9882, 0.2549],\n",
       "         [0.0000, 0.0000, 0.9882, 0.3098],\n",
       "         [0.2784, 0.9882, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.2784, 0.9882],\n",
       "         [0.0000, 0.0000, 0.2431, 0.9922],\n",
       "         [0.9686, 0.0353, 0.0000, 0.3059],\n",
       "         [0.1765, 0.9922, 0.0000, 0.0000],\n",
       "         [0.0000, 1.0000, 0.0824, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 1.0000],\n",
       "         [0.0000, 0.0863, 0.9882, 0.0863],\n",
       "         [0.0000, 0.9922, 0.0824, 0.0000],\n",
       "         [0.0000, 0.9922, 0.0824, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.9922],\n",
       "         [0.0000, 0.0000, 0.0000, 0.9922],\n",
       "         [0.9922, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0549, 0.9882, 0.0000],\n",
       "         [0.9882, 0.0431, 0.0000, 0.0000],\n",
       "         [0.9882, 0.0431, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.9882],\n",
       "         [0.0000, 0.0000, 0.0000, 0.9882],\n",
       "         [0.0000, 0.9882, 0.0000, 0.0000]]),\n",
       " 9)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist_dataset.__getitem__(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d515ac3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_one = 0\n",
    "class_two = 0\n",
    "binary_dataset = []\n",
    "\n",
    "for image in reduced_mnist:\n",
    "    if image[1] == 0 and class_one < 200:\n",
    "        binary_dataset.append(image)\n",
    "        class_one +=1 \n",
    "    elif image[1] == 1 and class_two < 200:\n",
    "        binary_dataset.append(image)\n",
    "        class_two += 1\n",
    "        \n",
    "    if class_one == 200 and class_two == 200:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6dcef0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels = zip(*binary_dataset)\n",
    "mnist_images = torch.stack(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "02ee1992",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([400, 50, 4])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5b3ae5e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.4163)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean = torch.mean(mnist_images)\n",
    "mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a7b5434b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1968)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diffs = mnist_images - mean\n",
    "var = torch.mean(torch.pow(diffs, 2.0))\n",
    "var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fbe13f0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.4436)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "std = torch.pow(var, 0.5)\n",
    "std"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c90f06",
   "metadata": {},
   "source": [
    "### Training my model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3c2e6dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#quantum_processing = transforms.Compose([Quanv_2d(n_qubits, toy_chromosome)])\n",
    "training_zeros_ones = SampledDataset4Training(binary_dataset)\n",
    "training_loader = DataLoader(training_zeros_ones, batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "970ae5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(hqcnn.parameters(), lr=0.1, weight_decay=1e-4)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "#loss = loss_fn(output, mnist_labels)\n",
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f2fc876a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: tensor(0.4046)\n",
      "std: tensor(0.4424)\n"
     ]
    }
   ],
   "source": [
    "mean = 0.\n",
    "meansq = 0.\n",
    "for (data, label) in training_loader:\n",
    "    mean = data.mean()\n",
    "    meansq = (data**2).mean()\n",
    "\n",
    "std = torch.sqrt(meansq - mean**2)\n",
    "print(\"mean: \" + str(mean))\n",
    "print(\"std: \" + str(std))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "08f4aba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "post_processing = [data for (data, label) in training_loader]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8276ca39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 100, 50, 4])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "post_process = torch.stack(post_processing).squeeze(0)\n",
    "post_process.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3fe35fff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.4163)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_p = torch.mean(post_process)\n",
    "mean_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2bfbc050",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1968)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_diffs = post_process - mean_p\n",
    "p_var = torch.mean(torch.pow(p_diffs, 2.0))\n",
    "p_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3a8f31c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.4436)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_std = torch.pow(p_var, 0.5)\n",
    "p_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "545eb3c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6441)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phi_n = F.normalize(post_process.flatten(start_dim=1), dim=1)\n",
    "cos_sim = phi_n @ phi_n.T\n",
    "cos_sim.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "10146a35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passing through quanvolution layer...\n",
      "Quanvolution processing time: 2.356858999992255\n",
      "Epoch [1/4], Step [1/4], Loss: 0.7029, Acc: 0.4800\n",
      "Passing through quanvolution layer...\n",
      "Quanvolution processing time: 2.103883000003407\n",
      "Epoch [1/4], Step [2/4], Loss: 0.7010, Acc: 0.4850\n",
      "Passing through quanvolution layer...\n",
      "Quanvolution processing time: 2.0533047999924747\n",
      "Epoch [1/4], Step [3/4], Loss: 0.6972, Acc: 0.5000\n",
      "Passing through quanvolution layer...\n",
      "Quanvolution processing time: 2.0809785000019474\n",
      "Epoch [1/4], Step [4/4], Loss: 0.6976, Acc: 0.5000\n",
      "Epoch 1: Train Loss=0.6976, Train Acc=0.5000\n",
      "Passing through quanvolution layer...\n",
      "Quanvolution processing time: 2.122514400005457\n",
      "Epoch [2/4], Step [1/4], Loss: 0.7043, Acc: 0.4600\n",
      "Passing through quanvolution layer...\n",
      "Quanvolution processing time: 2.2003954000101658\n",
      "Epoch [2/4], Step [2/4], Loss: 0.6977, Acc: 0.4900\n",
      "Passing through quanvolution layer...\n",
      "Quanvolution processing time: 2.687404600001173\n",
      "Epoch [2/4], Step [3/4], Loss: 0.6979, Acc: 0.5000\n",
      "Passing through quanvolution layer...\n",
      "Quanvolution processing time: 2.397178500003065\n",
      "Epoch [2/4], Step [4/4], Loss: 0.6976, Acc: 0.5000\n",
      "Epoch 2: Train Loss=0.6976, Train Acc=0.5000\n",
      "Passing through quanvolution layer...\n",
      "Quanvolution processing time: 2.236737600003835\n",
      "Epoch [3/4], Step [1/4], Loss: 0.7139, Acc: 0.4100\n",
      "Passing through quanvolution layer...\n",
      "Quanvolution processing time: 2.281245800011675\n",
      "Epoch [3/4], Step [2/4], Loss: 0.7019, Acc: 0.4700\n",
      "Passing through quanvolution layer...\n",
      "Quanvolution processing time: 2.3876097000029404\n",
      "Epoch [3/4], Step [3/4], Loss: 0.6980, Acc: 0.4933\n",
      "Passing through quanvolution layer...\n",
      "Quanvolution processing time: 2.1581999999907566\n",
      "Epoch [3/4], Step [4/4], Loss: 0.6976, Acc: 0.5000\n",
      "Epoch 3: Train Loss=0.6976, Train Acc=0.5000\n",
      "Passing through quanvolution layer...\n",
      "Quanvolution processing time: 2.400881199995638\n",
      "Epoch [4/4], Step [1/4], Loss: 0.6944, Acc: 0.5200\n",
      "Passing through quanvolution layer...\n",
      "Quanvolution processing time: 2.300664800000959\n",
      "Epoch [4/4], Step [2/4], Loss: 0.6990, Acc: 0.4950\n",
      "Passing through quanvolution layer...\n",
      "Quanvolution processing time: 2.1583725000091363\n",
      "Epoch [4/4], Step [3/4], Loss: 0.6998, Acc: 0.4867\n",
      "Passing through quanvolution layer...\n",
      "Quanvolution processing time: 2.4282603000028757\n",
      "Epoch [4/4], Step [4/4], Loss: 0.6976, Acc: 0.5000\n",
      "Epoch 4: Train Loss=0.6976, Train Acc=0.5000\n"
     ]
    }
   ],
   "source": [
    "#simple_fc = ClassicalComponent(num_classes, input_size)\n",
    "train_model(hqcnn, training_loader, num_epochs, optimizer, loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "01040e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, num_epochs, optimizer, loss_fn):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  # Set model to training mode\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        epoch_loss = 0.0\n",
    "        total = 0\n",
    "        \n",
    "        \n",
    "        for i, (inputs, labels) in enumerate(train_loader):\n",
    "        \n",
    "            optimizer.zero_grad()  # Zero out previous gradients\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_fn(outputs, labels)  # Calculate loss\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "            \n",
    "            loss.backward()  # Backpropagate to calculate gradients\n",
    "            optimizer.step() # Update weights\n",
    "            \n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_loader)}], Loss: {running_loss/(i+1):.4f}, Acc: {correct / total:.4f}\")\n",
    "            \n",
    "        epoch_loss = running_loss / len(train_loader)\n",
    "        epoch_acc = correct / total\n",
    "        print(f\"Epoch {epoch+1}: Train Loss={epoch_loss:.4f}, Train Acc={epoch_acc:.4f}\")\n",
    "            # Print every 10 batches\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f07947b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b83bc791",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 200 into shape (28,28)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[54]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m     12\u001b[39m images, labels = \u001b[38;5;28mnext\u001b[39m(dataiter)\n\u001b[32m     14\u001b[39m img_grid = make_grid(images)\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m \u001b[43mmatplotlib_imshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_grid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mone_channel\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[54]\u001b[39m\u001b[32m, line 5\u001b[39m, in \u001b[36mmatplotlib_imshow\u001b[39m\u001b[34m(img, one_channel)\u001b[39m\n\u001b[32m      3\u001b[39m     img = img.mean(dim=\u001b[32m0\u001b[39m)\n\u001b[32m      4\u001b[39m img = img / \u001b[32m2\u001b[39m + \u001b[32m0.5\u001b[39m     \u001b[38;5;66;03m# unnormalize\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m npimg = \u001b[43mimg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m28\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m28\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m one_channel:\n\u001b[32m      7\u001b[39m     plt.imshow(npimg, cmap=\u001b[33m\"\u001b[39m\u001b[33mGreys\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mValueError\u001b[39m: cannot reshape array of size 200 into shape (28,28)"
     ]
    }
   ],
   "source": [
    "def matplotlib_imshow(img, one_channel=False):\n",
    "    if one_channel:\n",
    "        img = img.mean(dim=0)\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy().reshape(28, 28)\n",
    "    if one_channel:\n",
    "        plt.imshow(npimg, cmap=\"Greys\")\n",
    "    else:\n",
    "        plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "\n",
    "dataiter = iter(training_loader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "img_grid = make_grid(images)\n",
    "matplotlib_imshow(img_grid, one_channel=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69bdd567",
   "metadata": {},
   "source": [
    "### Testing tensor dimensions and quantum convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "704233ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 3, 2, 2])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction import image\n",
    "from torch import tensor\n",
    "\n",
    "input_size = 4\n",
    "dummy_input = torch.zeros(input_size, input_size, 2).numpy()\n",
    "patched_images = [image.extract_patches_2d(img, (patch_size, patch_size)) for img in dummy_input]\n",
    "#dummy_output = quanv.forward(dummy_input)\n",
    "\n",
    "tensor(patched_images).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f186a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import image\n",
    "from torch import tensor\n",
    "\n",
    "input_size = 28\n",
    "\n",
    "shape = (input_size, input_size)\n",
    "dummy_input = np.zeros(shape, dtype=float)\n",
    "patched_images = image.extract_patches_2d(dummy_input, (patch_size, patch_size)) \n",
    "#dummy_output = quanv.forward(dummy_input)\n",
    "n_patches, patch_h, patch_w = tensor(patched_images).shape\n",
    "patched_images = tensor(patched_images).contiguous().view(n_patches, patch_h*patch_w)\n",
    "image._extract_patches()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a4f9a68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0.], dtype=torch.float64)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patched_images[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f9b26f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([729, 4])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patched_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9251b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "myAnsatz = AnsatzSimulation(n_qubits)\n",
    "outputs = [myAnsatz.simulate_circuit(patch, 'rx', toy_chromosome) for patch in patched_images]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523e60ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1458, 4])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor(outputs).shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".genetic_circuits (3.12.10)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
